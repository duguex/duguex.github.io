<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.2.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"tobedetermined.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"always","padding":18,"offset":12},"hljswrap":true,"copycode":{"enable":true,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="TODO">
<meta property="og:url" content="http://tobedetermined.com/index.html">
<meta property="og:site_name" content="TODO">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Mingzhe Liu">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://tobedetermined.com/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>TODO</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">TODO</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mingzhe Liu</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/duguex" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;duguex" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:duguex@126.com" title="E-Mail → mailto:duguex@126.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/07/30/NISQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/30/NISQ/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-07-30 01:22:40 / Modified: 00:45:20" itemprop="dateCreated datePublished" datetime="2024-07-30T01:22:40+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="利用量子计算机解决强关联体系的计算问题"><a href="#利用量子计算机解决强关联体系的计算问题" class="headerlink" title="利用量子计算机解决强关联体系的计算问题"></a>利用量子计算机解决强关联体系的计算问题</h1><hr>
<ul>
<li><p>基于密度泛函的第一性原理计算在处理强关联体系的电子态时的精度不足</p>
</li>
<li><p>相关方法计算成本很高 (dynamical mean-field theory and quantum Monte-Carlo)</p>
</li>
<li><p>量子计算机可以高效地对相关体系进行模拟，但是量子比特数量有限，只能对少数原子进行第一性模拟。</p>
</li>
</ul>
<p>因此采用嵌入方法，将需要准确性的部分用量子比特进行模拟，其余部分用经典计算机进行模拟，可以在保证精度的同时降低计算成本。</p>
<hr>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>先构造活性空间的有效Hamiltonians, 然后进行量子模拟</p>
<p><img src="/image.png" alt="Alt text"><br>npj Computational Materials, 02 July 2020</p>
<hr>
<h1 id="Quantum-embedding"><a href="#Quantum-embedding" class="headerlink" title="Quantum embedding"></a>Quantum embedding</h1><ul>
<li>活性空间的有效Hamiltonians可以通过第一性原理计算得到</li>
<li>主要反映背景对活性空间电子态的影响</li>
<li>方法较多，有开源程序</li>
</ul>
<hr>
<p><img src="/image-4.png" alt="Alt text"><br>Nat Comput Sci 2, 424–432 (2022)</p>
<hr>
<h1 id="Quantum-simulations"><a href="#Quantum-simulations" class="headerlink" title="Quantum simulations"></a>Quantum simulations</h1><p>两种方法 quantum phase estimation algorithm (PEA) and variational quantum eigensolvers (VQE)</p>
<p>PEA: 将一个系统的哈密顿量映射到一个量子比特的哈密顿量(Jordan–Wigner transformation)，然后使用量子相位估计算法来估计这个哈密顿量的本征值。PEA 需要一个完全的量子计算机来运行，而且对量子比特的质量和数量都有很高的要求。因此，尽管 PEA 可以提供非常精确的结果，但是在当前的噪声中等规模的量子（NISQ）设备上，它很难实现。</p>
<hr>
<p>VQE: 用于找到一个哈密顿量的基态能量。可以在有噪声的量子硬件上有效地运行。基本思想是使用一个参数化的量子电路（也称为 ansatz）来准备一个量子态，然后在这个态上测量哈密顿量的期望值。通过改变量子电路的参数最小化测量到的期望值，从而找到哈密顿量的近似基态。</p>
<hr>
<p><img src="/image-1.png" alt="Alt text"></p>
<hr>
<p><img src="/image-2.png" alt="Alt text"></p>
<hr>
<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2><p>结合量子计算机与传统计算机准确处理强关联体系</p>
<h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul>
<li>测试目前能用的量子计算机能否胜任相关模拟</li>
<li>测试quantum embedding的有效性</li>
<li>对目前关注的问题&#x2F;体系进行测试<br><img src="/image-8.png" alt="Alt text">  <img src="/image-6.png" alt="Alt text"></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/07/30/bigdealmd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/30/bigdealmd/" class="post-title-link" itemprop="url">Untitled</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2024-07-30 01:22:40 / Modified: 00:45:20" itemprop="dateCreated datePublished" datetime="2024-07-30T01:22:40+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>整合分散的计算资源</p>
<p>tc4600:</p>
<ul>
<li><ol>
<li>smallopa 2680v4 2*14 cpus&#x2F;8 channel</li>
</ol>
</li>
<li><ol start="2">
<li>smallib 2680v3 2*12 cpus&#x2F;8 channel</li>
</ol>
</li>
<li><ol start="3">
<li>fat144 8860v4 8*18 cpus&#x2F;32 channel</li>
</ol>
</li>
</ul>
<p>微尺度:</p>
<ul>
<li><ol>
<li>compute Gold 6248R 2*24 cpus&#x2F;12 channel</li>
</ol>
</li>
</ul>
<p>瀚海22:</p>
<ul>
<li><ol>
<li>ARM-CPU 鲲鹏920 2*32 cpus&#x2F;16 channel</li>
</ol>
</li>
</ul>
<hr>
<p>home-server:</p>
<ul>
<li><ol>
<li>p100 3*Tesla P100</li>
</ol>
</li>
<li><ol start="2">
<li>7R12 7R12 48 cpus&#x2F;8 channel</li>
</ol>
</li>
</ul>
<p>主要困难:</p>
<ul>
<li><ol>
<li>不同集群在不同网络环境</li>
</ol>
</li>
<li><ol start="2">
<li>部分集群防护等级高，禁止使用密钥</li>
</ol>
</li>
</ul>
<hr>
<p>特点: </p>
<ul>
<li>通过ssh协议实现跨集群操作</li>
<li>不在任何集群登录节点运行程序</li>
<li>异步执行任务</li>
</ul>
<p>应用场景：</p>
<ol>
<li>队列负载监控</li>
</ol>
<p><img src="/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-04-17%20072603.png" alt="alt text"></p>
<hr>
<ol start="2">
<li>异地提交作业</li>
</ol>
<ul>
<li><ol>
<li>本地集群：将运行程序和当前目录写入特定文件</li>
</ol>
</li>
<li><ol start="2">
<li>远程守护进程：监控特定文件，将作业提交到远程集群</li>
</ol>
</li>
<li><ol start="3">
<li>远程集群：运行作业, callback</li>
</ol>
</li>
</ul>
<p><img src="/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202024-04-17%20074418.png" alt="alt text"></p>
<ol start="3">
<li>作业状态监控(待完成)<br>通过邮件，webapp或者群机器人提供作业状态查询服务</li>
</ol>
<hr>
<p>调用方式</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">future server_name <span class="built_in">command</span></span><br><span class="line">future p100        vasp</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/07/05/spin-lattice/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/07/05/spin-lattice/" class="post-title-link" itemprop="url">spin-lattice</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-07-05 14:18:33" itemprop="dateCreated datePublished" datetime="2024-07-05T14:18:33+08:00">2024-07-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:21" itemprop="dateModified" datetime="2024-07-30T00:45:21+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>We proceed with the ab initio calculation of the spin-lattice relaxation rates by first computing the spin-phonon matrix elements up to second order, then applying Fermi’s golden rule, and finally taking the continuum limit, converting sums over matrix elements into integrals over spectral functions. We note that we apply Fermi’s golden rule with the random phase approximation such that interference terms are dropped.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/16/blender/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/16/blender/" class="post-title-link" itemprop="url">blender</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-16 19:30:56" itemprop="dateCreated datePublished" datetime="2024-06-16T19:30:56+08:00">2024-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Beautiful Atoms<br><a target="_blank" rel="noopener" href="https://beautiful-atoms.readthedocs.io/en/latest/getting_started/python.html">https://beautiful-atoms.readthedocs.io/en/latest/getting_started/python.html</a><br>from ase.build import molecule<br>from batoms_api import render<br>atoms &#x3D; molecule(“H2O”)<br>render_input &#x3D; {“viewport”: [1, 1, 0], “engine”: “cycles”, “output”: “h2o.png”,}<br>render(atoms, render_input&#x3D;render_input)</p>
<p>pymol<br>OVITO, ATOMEYE<br>python的ASE包生成pov文件然后用povray渲染<br>CYLview</p>
<h1 id="beautiful-atoms-blender"><a href="#beautiful-atoms-blender" class="headerlink" title="beautiful atoms &#x2F; blender"></a>beautiful atoms &#x2F; blender</h1><h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2><ul>
<li>run Blender as administrator</li>
</ul>
<p>import pip<br>pip.main([‘install’, ‘ase’])<br>pip.main([‘install’, ‘scikit-image’])</p>
<ul>
<li>Download the latest version (beautiful-atoms.zip).</li>
<li>Extract the file, move the folder batoms to Blender addons folder, could be C:\Program Files\Blender Foundation\Blender 3.4\3.4\scripts\addons\ or %USERPROFILE%\AppData\Roaming\Blender Foundation\Blender\3.4\scripts\addons.</li>
<li>Enable the addon in the Preferences setting. Please open a Blender Python console, and run the following code to enable the batoms:</li>
</ul>
<p>import addon_utils<br>import bpy<br>addon_utils.enable(‘batoms’, default_set&#x3D;True)<br>bpy.context.preferences.view.use_translate_new_dataname &#x3D; False<br>bpy.ops.wm.save_userpref()</p>
<p>pip.main([‘install’, ‘batoms’])<br>pip.main([‘install’, ‘batoms-api’])<br>pip.main([‘install’, ‘pymatgen’])</p>
<h2 id="how-to-read-POSCAR-or-SPIN-vasp"><a href="#how-to-read-POSCAR-or-SPIN-vasp" class="headerlink" title="how to read POSCAR or SPIN.vasp"></a>how to read POSCAR or SPIN.vasp</h2><p>from batoms.bio import read<br>sic &#x3D; read(“POSCAR”)</p>
<p>from pymatgen.io.vasp.outputs import VolumetricData<br>from batoms import Batoms</p>
<p>poscar, data, data_aug &#x3D; VolumetricData.parse_file(r”D:\onedrive\general_scripts_of_vasp\metadata\nosym\attempt_1107\spin\spin_hk.vasp”)<br>batoms &#x3D; Batoms(‘batoms’, from_pymatgen &#x3D; poscar.structure)<br>batoms.volumetric_data[‘chgcar’] &#x3D; data[‘total’]<br>batoms.isosurface.settings[“1”] &#x3D; {‘level’: 14.0, ‘color’: [1, 1, 0, 0.3]}<br>batoms.isosurface.draw()</p>
<h2 id="how-to-cut-slice"><a href="#how-to-cut-slice" class="headerlink" title="how to cut slice"></a>how to cut slice</h2><p>batoms.lattice_plane.setting[(0, 1, 0)] &#x3D; {“distance”: 6, “slicing”: True}<br>batoms.get_image(viewport &#x3D; [0, 1, 0], engine &#x3D; “cycles”, output&#x3D; r”D:\onedrive\general_scripts_of_vasp\metadata\nosym\attempt_1107\spin\h2o.png”)</p>
<h2 id="display-boundary"><a href="#display-boundary" class="headerlink" title="display boundary"></a>display boundary</h2><p>batoms.boundary&#x3D; np.array([[-0.05,1.05],[-0.05,1.05],[-0.05,1.05]])<br>batoms.wrap &#x3D; True</p>
<h2 id="change-color"><a href="#change-color" class="headerlink" title="change color"></a>change color</h2><p>ch4[“H”].color &#x3D; [0, 0, 0.8, 0.2]</p>
<p>batoms.bond.settings[(“C”, “H”)].color1 &#x3D; [0.8, 0.1, 0.3, 0.5]<br>batoms.bond.settings[(“C”, “H”)].color2 &#x3D; [0.1, 0.3, 0.2, 1.0]<br>batoms.model_style &#x3D; 1</p>
<h2 id="change-size"><a href="#change-size" class="headerlink" title="change size"></a>change size</h2><p>batoms[0].scale &#x3D; 2<br>batoms[240].scale &#x3D; 2</p>
<h2 id="dict-as-attribute"><a href="#dict-as-attribute" class="headerlink" title="dict as attribute"></a>dict as attribute</h2><p>from ase.build import bulk<br>from batoms import Batoms<br>import numpy as np<br>au &#x3D; bulk(“Au”, cubic&#x3D;True)*[5, 5, 5]<br>au &#x3D; Batoms(label &#x3D; “au”, from_ase &#x3D; au)</p>
<h2 id="add-z-coordinate-as-a-new-attribute-normalized-it-to-0-1"><a href="#add-z-coordinate-as-a-new-attribute-normalized-it-to-0-1" class="headerlink" title="add z coordinate as a new attribute, normalized it to [0, 1]"></a>add z coordinate as a new attribute, normalized it to [0, 1]</h2><p>z &#x3D; au.positions[:, 2]<br>au.set_attributes({“z_coor”: (z-np.min(z))&#x2F;(np.max(z)-np.min(z)) })</p>
<h2 id="color-atoms-by-their-z-coordinate"><a href="#color-atoms-by-their-z-coordinate" class="headerlink" title="color atoms by their z coordinate"></a>color atoms by their z coordinate</h2><p>au.species.color_by_attribute(“z_coor”)<br>au.get_image(viewport &#x3D; [1, 0, 0], output &#x3D; “color_by_z_coordinate.png”)</p>
<h2 id="label"><a href="#label" class="headerlink" title="label"></a>label</h2><p>batoms.show_label &#x3D; ‘index’<br>batoms.show_label &#x3D; ‘species’<br>batoms.show_label &#x3D; ‘elements’<br>batoms.set_attributes({“charges”: np.zeros(5)})<br>batoms.show_label &#x3D; ‘charges’</p>
<h2 id="divide-element-into-groups"><a href="#divide-element-into-groups" class="headerlink" title="divide element into groups"></a>divide element into groups</h2><p>batoms.replace([1], “H_1”)<br>batoms.replace([0], “Si_V”)<br>batoms.replace([240], “C_V”)<br>ch4[“H_1”].color &#x3D; (1.0, 1.0, 0.0, 1.0)</p>
<h2 id="auto-divide-with-symmetry"><a href="#auto-divide-with-symmetry" class="headerlink" title="auto divide with symmetry"></a>auto divide with symmetry</h2><p>magnetite.auto_build_species()</p>
<h2 id="highlight"><a href="#highlight" class="headerlink" title="highlight"></a>highlight</h2><p>batoms.selects.add(“s1”, [2])<br>batoms.highlight.settings[“s1”] &#x3D; {“select”: “s1”, “scale”: 0.5, “color”: (0.5, 0.5, 0, 0.4)}<br>batoms.highlight.draw()<br>batoms.get_image(padding &#x3D; 3, output &#x3D; “highlight_batoms.png”)</p>
<h2 id="camera-centering-atom"><a href="#camera-centering-atom" class="headerlink" title="camera centering atom"></a>camera centering atom</h2><p>batoms.get_image(padding &#x3D; 3,viewport &#x3D; [0, 1, 0], center&#x3D;batoms.positions[0], output &#x3D; r”D:\onedrive\general_scripts_of_vasp\metadata\nosym\attempt_1107\spin\look_at.png”)</p>
<h2 id="hide-atom"><a href="#hide-atom" class="headerlink" title="hide atom"></a>hide atom</h2><p>batoms[0].show&#x3D;False<br>batoms[np.array(batoms.elements) &#x3D;&#x3D; “H”].show &#x3D; False<br>batoms[batoms.positions[:,1]&lt;0.48<em>15.479473].show &#x3D; False<br>batoms[batoms.positions[:,1]&gt;0.52</em>15.479473].show &#x3D; False</p>
<p>batoms.get_image(padding &#x3D; 3, output &#x3D; “hide_batoms.png”)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/16/qc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/16/qc/" class="post-title-link" itemprop="url">qc</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-16 01:45:58" itemprop="dateCreated datePublished" datetime="2024-06-16T01:45:58+08:00">2024-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="quantum-environment"><a href="#quantum-environment" class="headerlink" title="quantum environment"></a>quantum environment</h1><p>conda install conda-forge::qiskit-ibm-runtime qutip qiskit matplotlib<br>conda install conda-forge::pylatexenc<br>conda install -c anaconda pip<br>pip install qutip-qip</p>
<p>变分量子本征求解器（variational quantum eigensolver，VQE）简介<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361420114">https://zhuanlan.zhihu.com/p/361420114</a></p>
<p>千变万化的Ansatz<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/538936873">https://zhuanlan.zhihu.com/p/538936873</a></p>
<p>变分量子算法（variational quantum algorithms, VQA）简介<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/358552587">https://zhuanlan.zhihu.com/p/358552587</a></p>
<p>Jordan-Wigner 变换 速查页<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386386413">https://zhuanlan.zhihu.com/p/386386413</a></p>
<p>Fermions and Jordan-Wigner String<br><a target="_blank" rel="noopener" href="https://itensor.org/docs.cgi?page=tutorials/fermions">https://itensor.org/docs.cgi?page=tutorials/fermions</a><br><a target="_blank" rel="noopener" href="https://itensor.org/docs.cgi?page=tutorials&vers=cppv3">https://itensor.org/docs.cgi?page=tutorials&amp;vers=cppv3</a></p>
<p>自动生成 矩阵乘积算符 的算法实现（github-AutoMPO）<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/385274056">https://zhuanlan.zhihu.com/p/385274056</a><br><a target="_blank" rel="noopener" href="https://github.com/Haokai-Zhang">https://github.com/Haokai-Zhang</a></p>
<p>变分量子算法（variational quantum algorithms, VQA）简介<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/358552587">https://zhuanlan.zhihu.com/p/358552587</a></p>
<p>The Kohn-Sham equation<br><a target="_blank" rel="noopener" href="https://uclnatsci.github.io/2022/NSCI0011/DFT/Equations.html">https://uclnatsci.github.io/2022/NSCI0011/DFT/Equations.html</a></p>
<p>DMFT</p>
<p>TDDFT<br><a target="_blank" rel="noopener" href="https://wiki.fysik.dtu.dk/gpaw/tutorialsexercises/opticalresponse/tddft/lcaotddft.html">https://wiki.fysik.dtu.dk/gpaw/tutorialsexercises/opticalresponse/tddft/lcaotddft.html</a></p>
<p>qutip<br>Hierarchical Equations of Motion<br><a target="_blank" rel="noopener" href="https://qutip.org/docs/latest/guide/guide-heom.html">https://qutip.org/docs/latest/guide/guide-heom.html</a></p>
<p>本源量子<br><a target="_blank" rel="noopener" href="https://vqnet20-tutorial.readthedocs.io/en/main/index.html">https://vqnet20-tutorial.readthedocs.io/en/main/index.html</a><br><a target="_blank" rel="noopener" href="https://qpanda-tutorial.readthedocs.io/zh/latest/?badge=latest">https://qpanda-tutorial.readthedocs.io/zh/latest/?badge=latest</a><br><a target="_blank" rel="noopener" href="https://pyqpanda-toturial.readthedocs.io/zh/latest/?badge=latest">https://pyqpanda-toturial.readthedocs.io/zh/latest/?badge=latest</a></p>
<h1 id="No-module-named-‘qiskit-extensions-unitary’"><a href="#No-module-named-‘qiskit-extensions-unitary’" class="headerlink" title="No module named ‘qiskit.extensions.unitary’"></a>No module named ‘qiskit.extensions.unitary’</h1><p>According to this PR (Qiskit&#x2F;qiskit#10725), will receive the error ModuleNotFoundError: No module named ‘qiskit.extensions.unitary’</p>
<p>from qiskit.extensions.unitary import UnitaryGate<br>should be replaced with</p>
<p>from qiskit.circuit.library import UnitaryGate<br>for Qiskit version 0.45.0</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/16/qe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/16/qe/" class="post-title-link" itemprop="url">qe</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-16 00:48:30" itemprop="dateCreated datePublished" datetime="2024-06-16T00:48:30+08:00">2024-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ubuntu-container-install-qe-and-west"><a href="#ubuntu-container-install-qe-and-west" class="headerlink" title="ubuntu container install qe and west"></a>ubuntu container install qe and west</h1><h2 id="install"><a href="#install" class="headerlink" title="install"></a>install</h2><p>apt install cmake nano git build-essential gfortran libopenblas-dev libfftw3-dev libscalapack-openmpi-dev libopenmpi-dev wget python3-dev python3-pip lrzsz python-is-python3</p>
<p>git clone -b ‘qe-7.2’ –single-branch –depth 1 <a target="_blank" rel="noopener" href="https://gitlab.com/QEF/q-e.git">https://gitlab.com/QEF/q-e.git</a> qe-7.2<br>cd qe-7.2<br>git clone -b ‘v5.5.0’ –single-branch –depth 1 <a target="_blank" rel="noopener" href="http://greatfire.uchicago.edu/west-public/West.git">http://greatfire.uchicago.edu/west-public/West.git</a> West<br>.&#x2F;configure -enable-openmp</p>
<p>–enable-parallel	compile for parallel (MPI) execution if possible (yes)<br>–enable-openmp	compile for OpenMP execution if possible (no)<br>–enable-static	produce static executables, arger but more portable (no)<br>–enable-shared	produce objects that are suitable for shared libraries (no)<br>–enable-debug	compile with debug flags (no)<br>–enable-pedantic	compile with gfortran pedantic flags on (no)<br>–enable-signals	enable signal trapping (no)</p>
<p>make all -j 8<br>cd West<br>edit Pytools&#x2F;Makefile, replace install: with - pip install .<br>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>make all -j 8</p>
<h2 id="environment-variable"><a href="#environment-variable" class="headerlink" title="environment variable"></a>environment variable</h2><p>export PATH&#x3D;$PATH:&#x2F;opt&#x2F;qe-7.2&#x2F;bin<br>export OMPI_ALLOW_RUN_AS_ROOT&#x3D;1<br>export OMPI_ALLOW_RUN_AS_ROOT_CONFIRM&#x3D;1</p>
<h1 id="compile-qe"><a href="#compile-qe" class="headerlink" title="compile qe"></a>compile qe</h1><p>module load libxc&#x2F;4.3.4_intel2017update4 hdf5&#x2F;1.10.5&#x2F;intel2018.update3<br>&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-7.0</p>
<p>&#x2F;opt&#x2F;libxc&#x2F;4.3.4_intel2017update4<br>&#x2F;opt&#x2F;hdf5&#x2F;1.10.5&#x2F;intel&#x2F;2018.3.222</p>
<p>.&#x2F;configure MPIF90&#x3D;mpiifort CC&#x3D;icc F77&#x3D;ifort –with-scalapack&#x3D;intel –with-libxc&#x3D;yes –with-libxc-prefix&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;libxc&#x2F;4.3.4 –with-libxc-include&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;libxc&#x2F;4.3.4&#x2F;include –with-hdf5&#x3D;yes –with-hdf5-libs&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;hdf5&#x2F;1.12.2_2019&#x2F;lib –with-hdf5-include&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;hdf5&#x2F;1.12.2_2019&#x2F;include</p>
<p>.&#x2F;configure –prefix&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;libxc&#x2F;4.3.4 FC&#x3D;ifort CC&#x3D;icc</p>
<p>make<br>make check<br>make install</p>
<p>vasp hdf5</p>
<p>&#x2F;home&#x2F;phys&#x2F;qif&#x2F;hdf&#x2F;1.12.2<br>&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-7.0&#x2F;bin&#x2F;pw.x</p>
<p>mpiifort -O2 -assume byterecl -g -traceback -nomodule -fpp -D__DFTI -D__LIBXC -D__MPI -D__SCALAPACK -D__HDF5 -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;include -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;FoX&#x2F;finclude -I&#x2F;opt&#x2F;intel&#x2F;compilers_and_libraries_2017.4.196&#x2F;linux&#x2F;mkl&#x2F;include -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;libxc&#x2F;4.2.3&#x2F;include -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;hdf&#x2F;1.12.2&#x2F;include -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;upflib -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;Modules -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;FFTXlib -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;LAXlib -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;UtilXlib -I&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;&#x2F;FoX&#x2F;finclude -I..&#x2F;ELPA&#x2F;src -c plugin_arguments.f90</p>
<p>module purge;module load intel&#x2F;2017.update4 intelmpi&#x2F;2017.update4 mkl&#x2F;2017.update4</p>
<p>bsub -q test -n 20 -m k802 -o %J.log -e %J.err mpijob “&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;bin&#x2F;pw.x&lt;pw.in”<br>bsub -q smallopa -n 56 -o %J.log -e %J.err mpijob “&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;bin&#x2F;pw.x&lt;pw.in”<br>bsub -q test -n 20 -m k802 -o %J.log -e %J.err mpijob “&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;bin&#x2F;gipaw.x&lt;hy.in” &gt;hy.out<br>bsub -q test -n 20 -o %J.log mpijob “pw.x&lt;pwscf.in” &gt; pwscf.out<br>#bsub -q ckduan -n 24 -o %J.log -e %J.err mpijob pyzfs –wfcfmt qeh5<br>#bsub -q ckduan -n 24 -o %J.log -e %J.err mpirun pyzfs –wfcfmt qeh5<br>bsub -q smallopa -n 56 -o %J.log -e %J.err mpiexec pyzfs –wfcfmt qeh5 –prefix cr<br>bsub -q test -n 20 -m k802 -o %J.log -e %J.err mpijob “&#x2F;home&#x2F;phys&#x2F;qif&#x2F;qe-6.7&#x2F;bin&#x2F;pw.x&lt;pw.in” &gt;pw.out</p>
<p>bsub -q ckduan -n 24 -o %J.log mpirun python run.py </p>
<p>.&#x2F;configure –prefix&#x3D;&#x2F;home&#x2F;phys&#x2F;qif&#x2F;hdf5&#x2F;1.12.2_2019 –enable-fortran –enable-parallel –enable-shared CC&#x3D;mpiicc FC&#x3D;mpiifort CXX&#x3D;mpiicpc</p>
<p>libhdf5.a<br>libhdf5.la<br>libhdf5.settings<br>libhdf5.so<br>libhdf5.so.200<br>libhdf5.so.200.2.0</p>
<p>libhdf5_fortran.a<br>libhdf5_fortran.la<br>libhdf5_fortran.so<br>libhdf5_fortran.so.200<br>libhdf5_fortran.so.200.1.1</p>
<p>libhdf5_hl.a<br>libhdf5_hl.la<br>libhdf5_hl.so<br>libhdf5_hl.so.200<br>libhdf5_hl.so.200.1.0</p>
<p>libhdf5_hl_fortran.a<br>libhdf5_hl_fortran.so</p>
<p>libhdf5hl_fortran.a<br>libhdf5hl_fortran.la<br>libhdf5hl_fortran.so<br>libhdf5hl_fortran.so.200<br>libhdf5hl_fortran.so.200.0.2</p>
<p>-lhdf5hl_fortran -lhdf5_hl -lhdf5_fortran -lhdf5</p>
<h2 id="tutorial-wget-replace-http-www-with-https"><a href="#tutorial-wget-replace-http-www-with-https" class="headerlink" title="tutorial wget replace http://www with https:&#x2F;&#x2F;"></a>tutorial wget replace <a target="_blank" rel="noopener" href="http://www/">http://www</a> with https:&#x2F;&#x2F;</h2><p><a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/wfreq.json">https://west-code.org/doc/training/nv_diamond/wfreq.json</a>  good<br><a target="_blank" rel="noopener" href="http://www.west-code.org/doc/training/nv_diamond/wfreq.in">http://www.west-code.org/doc/training/nv_diamond/wfreq.in</a> not good</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/pw.in">https://west-code.org/doc/training/silane/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/wstat.in">https://west-code.org/doc/training/silane/wstat.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/wstat.json">https://west-code.org/doc/training/silane/wstat.json</a> -O silane.wstat.save&#x2F;wstat.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/wfreq.in">https://west-code.org/doc/training/silane/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/wfreq.json">https://west-code.org/doc/training/silane/wfreq.json</a> -O silane.wfreq.save&#x2F;wfreq.json</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/silane/wfreq_spec.in">https://west-code.org/doc/training/silane/wfreq_spec.in</a></p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/pw.in">https://west-code.org/doc/training/nv_diamond/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/wstat.in">https://west-code.org/doc/training/nv_diamond/wstat.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/wfreq.in">https://west-code.org/doc/training/nv_diamond/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/wfreq.json">https://west-code.org/doc/training/nv_diamond/wfreq.json</a> -O nv_diamond.wfreq.save&#x2F;wfreq.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/westpp.in">https://west-code.org/doc/training/nv_diamond/westpp.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond/nv_diamond.westpp.save.tar.gz">https://west-code.org/doc/training/nv_diamond/nv_diamond.westpp.save.tar.gz</a></p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/pw.in">https://west-code.org/doc/training/nv_diamond_63/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wstat.in">https://west-code.org/doc/training/nv_diamond_63/wstat.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.in">https://west-code.org/doc/training/nv_diamond_63/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.json">https://west-code.org/doc/training/nv_diamond_63/wfreq.json</a> -O west.wfreq.save&#x2F;wfreq.json</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/westpp.json">https://west-code.org/doc/training/nv_diamond_63/westpp.json</a> -O west.westpp.save&#x2F;westpp.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/westpp_2.json">https://west-code.org/doc/training/nv_diamond_63/westpp_2.json</a> -O west.westpp.save&#x2F;westpp.json</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/pw.in">https://west-code.org/doc/training/C60_pdep/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/nscf.in">https://west-code.org/doc/training/C60_pdep/nscf.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/wstat.in">https://west-code.org/doc/training/C60_pdep/wstat.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/wbse_init.in">https://west-code.org/doc/training/C60_pdep/wbse_init.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/wbse.in">https://west-code.org/doc/training/C60_pdep/wbse.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/wfreq.json">https://west-code.org/doc/training/C60_pdep/wfreq.json</a> -O west.wfreq.save&#x2F;wfreq.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60_pdep/wbse.json">https://west-code.org/doc/training/C60_pdep/wbse.json</a> -O west.wbse.save&#x2F;wbse.json</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/pw.in">https://west-code.org/doc/training/C60/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/nscf.in">https://west-code.org/doc/training/C60/nscf.in</a></p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/qb.in">https://west-code.org/doc/training/C60/qb.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/qb2.in">https://west-code.org/doc/training/C60/qb2.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/wstat.in">https://west-code.org/doc/training/C60/wstat.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/wfreq.in">https://west-code.org/doc/training/C60/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/wbse_init.in">https://west-code.org/doc/training/C60/wbse_init.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/wbse.in">https://west-code.org/doc/training/C60/wbse.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/C60/wbse.json">https://west-code.org/doc/training/C60/wbse.json</a> -O west.wbse.save&#x2F;wbse.json</p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/pw.in">https://west-code.org/doc/training/nv_diamond_63/pw.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/pp.in">https://west-code.org/doc/training/nv_diamond_63/pp.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.in">https://west-code.org/doc/training/nv_diamond_63/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.json">https://west-code.org/doc/training/nv_diamond_63/wfreq.json</a> -O west.wfreq.save&#x2F;wfreq.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B087.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B087.cube</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B122.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B122.cube</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B123.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B123.cube</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B126.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B126.cube</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B127.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B127.cube</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B128.cube">https://west-code.org/doc/training/nv_diamond_63/wfct_K001_B128.cube</a></p>
<p>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.in">https://west-code.org/doc/training/nv_diamond_63/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.json">https://west-code.org/doc/training/nv_diamond_63/wfreq.json</a> -O west.wfreq.save&#x2F;wfreq.json<br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.in">https://west-code.org/doc/training/nv_diamond_63/wfreq.in</a><br>wget -N -q <a target="_blank" rel="noopener" href="https://west-code.org/doc/training/nv_diamond_63/wfreq.json">https://west-code.org/doc/training/nv_diamond_63/wfreq.json</a> -O west.wfreq.save&#x2F;wfreq.json</p>
<h2 id="test"><a href="#test" class="headerlink" title="test"></a>test</h2><p>mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings pw.x -i pw.in | tee pw.out<br>mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings wstat.x -nimage 48 -npool 1 -nbgrp 1 -i wstat.in | tee wstat.out<br>mpirun -np 12 –map-by ppr:12:socket:PE&#x3D;4 –bind-to core -x OMP_NUM_THREADS&#x3D;4 -x OMP_STACKSIZE&#x3D;10g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings wfreq.x -nimage 3 -npool 2 -nbgrp 2 -i wfreq.in | tee wfreq.out</p>
<h1 id="install-westpy"><a href="#install-westpy" class="headerlink" title="install westpy"></a>install westpy</h1><p>pip install .</p>
<h1 id="build-singularity-container"><a href="#build-singularity-container" class="headerlink" title="build singularity container"></a>build singularity container</h1><p>sudo docker commit west2 west:5.5.0<br>sudo docker tag west:5.5.0 192.168.1.111:5000&#x2F;west:5.5.0<br>sudo docker push 192.168.1.111:5000&#x2F;west:5.5.0</p>
<p>docker tag vasp_gpu:12.3.86 172.29.10.87:5000&#x2F;vasp_gpu:12.3.86<br>docker push 172.29.10.87:5000&#x2F;vasp_gpu:12.3.86</p>
<h1 id="test-singularity-container"><a href="#test-singularity-container" class="headerlink" title="test singularity container"></a>test singularity container</h1><p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings &#x2F;opt&#x2F;qe-7.2&#x2F;bin&#x2F;pw.x -i pw.in | tee pw.out</p>
<p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings &#x2F;opt&#x2F;qe-7.2&#x2F;bin&#x2F;wstat.x -nimage 48 -i wstat.in | tee wstat.out</p>
<p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings wfreq.x -i wfreq_spec.in | tee wfreq_spec.out</p>
<p>singularity run ~&#x2F;container&#x2F;west python plot.py</p>
<h1 id="install-qbox-in-centos8-container"><a href="#install-qbox-in-centos8-container" class="headerlink" title="install qbox in centos8 container"></a>install qbox in centos8 container</h1><p>sudo docker run -it –name qbox centos</p>
<h2 id="switch-centos-repo-to-vault"><a href="#switch-centos-repo-to-vault" class="headerlink" title="switch centos repo to vault"></a>switch centos repo to vault</h2><p>sed -i -e “s&#x2F;mirrorlist&#x3D;&#x2F;#mirrorlist&#x3D;&#x2F;g” &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Linux-<em>.repo;<br>sed -i -e “s&#x2F;#baseurl&#x3D;&#x2F;baseurl&#x3D;&#x2F;g” &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Linux-</em>.repo;<br>sed -i -e “s&#x2F;mirror.centos.org&#x2F;vault.centos.org&#x2F;g” &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Linux-*.repo;</p>
<p>yum clean all<br>yum makecache<br>yum update</p>
<p>yum install epel-release<br>yum install xerces-c xerces-c-devel<br>yum install lrzsz make bc<br>yum install openmpi openmpi-devel<br>export PATH&#x3D;$PATH:&#x2F;usr&#x2F;lib64&#x2F;openmpi&#x2F;bin<br>yum install lapack<br>dnf –enablerepo&#x3D;powertools -y install lapack-devel<br>yum install fftw fftw-devel<br>yum install scalapack-common scalapack-openmpi scalapack-openmpi-devel scalapack-openmpi-static<br>yum install libuuid libuuid-devel</p>
<p>git clone <a target="_blank" rel="noopener" href="https://github.com/qboxcode/qbox-public.git">https://github.com/qboxcode/qbox-public.git</a></p>
<h2 id="use-template-for-centos7"><a href="#use-template-for-centos7" class="headerlink" title="use template for centos7"></a>use template for centos7</h2><p> PLT&#x3D;Linux_x8664</p>
<p> PLTOBJECTS &#x3D; readTSC.o</p>
<p> CXX&#x3D;mpicxx<br> LD&#x3D;$(CXX)</p>
<p> PLTFLAGS +&#x3D; -DIA32 -D_LARGEFILE_SOURCE <br>             -D_FILE_OFFSET_BITS&#x3D;64 -DUSE_MPI -DSCALAPACK -DADD_ <br>             -DAPP_NO_THREADS -DXML_USE_NO_THREADS -DUSE_XERCES <br>             -DXERCESC_3 -DMPICH_IGNORE_CXX_SEEK -DUSE_UUID</p>
<p> FFT&#x3D;FFTW3</p>
<p>ifeq ($(FFT),FFTW2)<br> PLTFLAGS +&#x3D; -DUSE_FFTW2<br> LIBS +&#x3D; -lfftw<br>endif</p>
<p>ifeq ($(FFT),FFTW3)<br> PLTFLAGS +&#x3D; -DUSE_FFTW3<br> PLTFLAGS +&#x3D; -DFFTW3_2D<br> LIBS +&#x3D; -lfftw3<br>endif</p>
<p>ifeq ($(FFT),ESSL)<br>$(error ESSL library not available)<br>endif</p>
<p>ifeq ($(FFT),NOLIB)<br> PLTFLAGS +&#x3D; -DFFT_NOLIB<br>endif</p>
<p> CXXFLAGS&#x3D; -g -O3 -Wunused -D$(PLT) $(INCLUDE) $(PLTFLAGS) $(DFLAGS)<br> LIBS +&#x3D; -lpthread -lxerces-c -lscalapack -llapack -lblas -luuid<br> LDFLAGS &#x3D; $(LIBPATH) $(LIBS)</p>
<p>cd src<br>cp ..&#x2F;build&#x2F;centos7.mk <myplatform>.mk<br>export TARGET&#x3D;<myplatform><br>make -j 8</p>
<h2 id="build-singularity-container-and-test"><a href="#build-singularity-container-and-test" class="headerlink" title="build singularity container and test"></a>build singularity container and test</h2><p>sudo docker commit qbox qbox:rel1_76_1 &amp;&amp; sudo docker tag qbox:rel1_76_1 192.168.1.111:5000&#x2F;qbox:rel1_76_1 &amp;&amp; sudo docker push 192.168.1.111:5000&#x2F;qbox:rel1_76_1</p>
<p>singularity run ~&#x2F;container&#x2F;qbox mpirun -np 2 –map-by ppr:2:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;20g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings qb si64gs.i | tee si64gs.r</p>
<p>singularity run ~&#x2F;container&#x2F;qbox mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings qb &lt; qb.in | tee qb.out</p>
<h2 id="qbox-potential"><a href="#qbox-potential" class="headerlink" title="qbox potential"></a>qbox potential</h2><p>available at <a target="_blank" rel="noopener" href="http://quantum-simulation.org/index.htm">http://quantum-simulation.org/index.htm</a></p>
<h1 id="run-qe-and-west"><a href="#run-qe-and-west" class="headerlink" title="run qe and west"></a>run qe and west</h1><p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings pw.x -i pw.in | tee pw.out</p>
<p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings wstat.x -ni 3 -i wstat.in | tee wstat.out</p>
<p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings wfreq.x -ni 3 -i wfreq.in | tee wfreq.out</p>
<p>singularity run ~&#x2F;container&#x2F;west mpirun -np 48 –map-by ppr:48:socket:PE&#x3D;1 –bind-to core -x OMP_NUM_THREADS&#x3D;1 -x OMP_STACKSIZE&#x3D;2g -x OMP_PLACES&#x3D;cores -x OMP_PROC_BIND&#x3D;close –report-bindings westpp.x -ni 3 -i westpp.in | tee westpp.out</p>
<h1 id="westpy-error-with-Collection-class-in-signac-package"><a href="#westpy-error-with-Collection-class-in-signac-package" class="headerlink" title="westpy error with Collection class in signac package"></a>westpy error with Collection class in signac package</h1><p>signac remove Collection class in 2.0</p>
<p>conda install -c anaconda pip<br>pip install signac&#x3D;&#x3D;1.8.0</p>
<h1 id="compile-template-for-west"><a href="#compile-template-for-west" class="headerlink" title="compile template for west"></a>compile template for west</h1><h2 id="ALCF-Polaris"><a href="#ALCF-Polaris" class="headerlink" title="ALCF-Polaris"></a>ALCF-Polaris</h2><p>Polaris is a GPU-accelerated supercomputer located at Argonne National Laboratory, maintained by ALCF.</p>
<p>$ ssh <username>@polaris.alcf.anl.gov<br>Building WEST<br>WEST executables can be compiled using the following script (tested on June 14, 2023):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load nvhpc&#x2F;23.3<br>module load cray-libsci&#x2F;21.08.1.2<br>module load cray-python&#x2F;3.9.12.1</p>
<p>export MPICH_GPU_SUPPORT_ENABLED&#x3D;1<br>export CUDA_HOME&#x3D;&#x2F;opt&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.3&#x2F;cuda&#x2F;11.8</p>
<p>.&#x2F;configure –with-cuda&#x3D;$CUDA_HOME –with-cuda-runtime&#x3D;11.8 –with-cuda-cc&#x3D;80</p>
<p>#Manually edit make.inc:</p>
<p>#DFLAGS &#x3D; -D__PGI -D__CUDA -D__FFTW -D__MPI -D__GPU_MPI<br>#MPIF90 &#x3D; ftn<br>#F90 &#x3D; ftn<br>#CC &#x3D; cc<br>#LD &#x3D; ftn<br>#BLAS_LIBS &#x3D; #leave blank<br>#LAPACK_LIBS &#x3D; #leave blank</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”-L&#x2F;opt&#x2F;cray&#x2F;pe&#x2F;python&#x2F;3.9.12.1&#x2F;lib -lpython3.9”<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Polaris with 4 MPI ranks and 4 GPUs per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: The following environment variable is needed to work around a bug in ROMIO, Cray MPICH.</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”<br>Important: It is recommended to run the calculation from one of the Lustre file systems (&#x2F;grand or &#x2F;eagle instead of &#x2F;home).</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash -l<br>#PBS -l select&#x3D;1:system&#x3D;polaris<br>#PBS -l place&#x3D;scatter<br>#PBS -l walltime&#x3D;0:20:00<br>#PBS -l filesystems&#x3D;home:grand<br>#PBS -j oe<br>#PBS -q debug<br>#PBS -A <project_name><br>#PBS -N job_name</p>
<p>module load nvhpc&#x2F;23.3<br>module load cray-libsci&#x2F;21.08.1.2<br>module load cray-python&#x2F;3.9.12.1</p>
<p>export MPICH_GPU_SUPPORT_ENABLED&#x3D;1<br>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”</p>
<p>NNODES&#x3D;<code>wc -l &lt; $PBS_NODEFILE</code><br>NRANKS_PER_NODE&#x3D;$(nvidia-smi -L | wc -l)<br>NDEPTH&#x3D;8<br>NTHREADS&#x3D;1<br>NTOTRANKS&#x3D;$(( NNODES * NRANKS_PER_NODE ))</p>
<p>cd ${PBS_O_WORKDIR}</p>
<p>mpiexec -n ${NTOTRANKS} –ppn ${NRANKS_PER_NODE} –depth&#x3D;${NDEPTH} –cpu-bind depth –env OMP_NUM_THREADS&#x3D;${NTHREADS} -env OMP_PLACES&#x3D;threads .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ qsub run_west.sh</p>
<p>ALCF-Theta<br>Theta is a Cray XC40 located at Argonne National Laboratory, maintained by ALCF.</p>
<p>$ ssh <username>@theta.alcf.anl.gov<br>Building WEST<br>Start an interactive job and ssh to a compute node. The <project_name> must be replaced with an active project allocation.</p>
<p>#Start interactive job<br>$ qsub -I -n 1 -t 1:00:00 –attrs enable_ssh&#x3D;1 -q debug-cache-quad -A <project_name><br>  Connecting to thetamom1 for interactive qsub…<br>  Job routed to queue “debug-cache-quad”.<br>  Memory mode set to cache quad for queue debug-cache-quad<br>  Wait for job 266815 to start…<br>  Opening interactive session to 3835</p>
<p>#Get compute node number<br>$ echo $COBALT_PARTNAME<br>  3835</p>
<p>#Full name of compute node is nid + 5-digit node number<br>$ ssh nid03835</p>
<p>#Set up proxy for internet access<br>$ export HTTP_PROXY&#x3D;<a target="_blank" rel="noopener" href="http://theta-proxy.tmi.alcf.anl.gov:3128/">http://theta-proxy.tmi.alcf.anl.gov:3128</a><br>$ export HTTPS_PROXY&#x3D;<a target="_blank" rel="noopener" href="http://theta-proxy.tmi.alcf.anl.gov:3128/">http://theta-proxy.tmi.alcf.anl.gov:3128</a><br>$ export http_proxy&#x3D;<a target="_blank" rel="noopener" href="http://theta-proxy.tmi.alcf.anl.gov:3128/">http://theta-proxy.tmi.alcf.anl.gov:3128</a><br>$ export https_proxy&#x3D;<a target="_blank" rel="noopener" href="http://theta-proxy.tmi.alcf.anl.gov:3128/">http://theta-proxy.tmi.alcf.anl.gov:3128</a><br>Important: The ELPA eigensolver library is found to greatly improve the performance of Quantum ESPRESSO on Theta. ELPA can be installed following these steps (tested on February 3, 2023):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module unload cray-libsci<br>module load cray-python&#x2F;3.8.2.1</p>
<p>export CRAYPE_LINK_TYPE&#x3D;dynamic<br>export LD_LIBRARY_PATH&#x3D;$MKLROOT&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;intel&#x2F;compilers_and_libraries_2020.0.166&#x2F;linux&#x2F;compiler&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export FC&#x3D;ftn<br>export CC&#x3D;cc<br>export CXX&#x3D;CC</p>
<p>wget <a target="_blank" rel="noopener" href="https://elpa.mpcdf.mpg.de/software/tarball-archive/Releases/2022.11.001/elpa-2022.11.001.tar.gz">https://elpa.mpcdf.mpg.de/software/tarball-archive/Releases/2022.11.001/elpa-2022.11.001.tar.gz</a><br>tar zxf elpa-2022.11.001.tar.gz<br>cd elpa-2022.11.001</p>
<p>mkdir build<br>cd build</p>
<p>..&#x2F;configure –prefix&#x3D;$(pwd) LDFLAGS&#x3D;”$MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_scalapack_lp64.so -Wl,–start-group $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_lp64.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_sequential.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_core.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_blacs_intelmpi_lp64.so -Wl,–end-group” –disable-sse-assembly –disable-sse –disable-avx512 –enable-c-tests&#x3D;no</p>
<p>make -j 8<br>make install<br>WEST executables can be compiled using the following script (tested on February 3, 2023):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module unload cray-libsci<br>module load cray-python&#x2F;3.8.2.1</p>
<p>export CRAYPE_LINK_TYPE&#x3D;dynamic<br>export LD_LIBRARY_PATH&#x3D;$MKLROOT&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;intel&#x2F;compilers_and_libraries_2020.0.166&#x2F;linux&#x2F;compiler&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;python&#x2F;3.8.2.1&#x2F;lib:$LD_LIBRARY_PATH<br>export MPIF90&#x3D;ftn<br>export F90&#x3D;ftn<br>export CC&#x3D;cc<br>export DFLAGS&#x3D;”-D__DFTI -D__MPI -D__SCALAPACK -D__ELPA”<br>export BLAS_LIBS&#x3D;”-Wl,–start-group $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_lp64.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_thread.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_core.so -Wl,–end-group”<br>export LAPACK_LIBS&#x3D;”-Wl,–start-group $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_lp64.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_thread.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_core.so -Wl,–end-group”<br>export SCALAPACK_LIBS&#x3D;”$MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_scalapack_lp64.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_blacs_intelmpi_lp64.so”</p>
<p>#Edit ELPA installation path<br>.&#x2F;configure –enable-openmp –with-elpa-include&#x3D;&#x2F;path&#x2F;to&#x2F;elpa-2022.11.001&#x2F;build&#x2F;include&#x2F;elpa-2022.11.001&#x2F;modules –with-elpa-lib&#x3D;&#x2F;path&#x2F;to&#x2F;elpa-2022.11.001&#x2F;build&#x2F;lib&#x2F;libelpa.a</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”-L&#x2F;opt&#x2F;python&#x2F;3.8.2.1&#x2F;lib -lpython3.8”<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Theta with 64 MPI ranks per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: The following environment variable is needed to work around a bug in ROMIO, Cray MPICH.</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”<br>Important: It is recommended to run the calculation from one of the Lustre file systems (&#x2F;grand or &#x2F;eagle instead of &#x2F;home).</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#COBALT -n 2<br>#COBALT -t 00:20:00<br>#COBALT -q debug-cache-quad<br>#COBALT -A <project_name><br>#COBALT -O WEST</p>
<p>MPIRANKS_PERNODE&#x3D;64<br>MPIRANKS&#x3D;$((COBALT_PARTSIZE * MPIRANKS_PERNODE))<br>NTHREADS&#x3D;1<br>HT&#x3D;1</p>
<p>module unload cray-libsci<br>module load cray-python&#x2F;3.8.2.1</p>
<p>export LD_LIBRARY_PATH&#x3D;$MKLROOT&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;intel&#x2F;compilers_and_libraries_2020.0.166&#x2F;linux&#x2F;compiler&#x2F;lib&#x2F;intel64:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;opt&#x2F;python&#x2F;3.8.2.1&#x2F;lib:$LD_LIBRARY_PATH</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”</p>
<p>export OMP_NUM_THREADS&#x3D;$NTHREADS<br>aprun -n $MPIRANKS -N $MPIRANKS_PERNODE -cc depth -d $NTHREADS -j $HT .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>Make the script executable:</p>
<p>$ chmod 755 run_west.sh<br>Job submission is done with the following:</p>
<p>$ qsub run_west.sh</p>
<p>ANL-LCRC-Bebop<br>Bebop is an HPC cluster maintained by the Laboratory Computing Resource Center (LCRC) at Argonne National Laboratory.</p>
<p>$ ssh <username>@bebop.lcrc.anl.gov<br>Building WEST<br>WEST executables can be compiled using the following script (tested on December 21, 2022):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module purge<br>module load git&#x2F;2.31.1-6p7naeb<br>module load intel-oneapi&#x2F;2021.4.0.3422<br>module load anaconda3&#x2F;2021.05</p>
<p>export MPIF90&#x3D;mpiifort<br>export F90&#x3D;ifort<br>export CC&#x3D;icc<br>export SCALAPACK_LIBS&#x3D;”-lmkl_scalapack_lp64 -Wl,–start-group -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -Wl,–end-group”</p>
<p>.&#x2F;configure –with-scalapack&#x3D;intel –enable-openmp<br>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”-L&#x2F;gpfs&#x2F;fs1&#x2F;home&#x2F;software&#x2F;anaconda3&#x2F;2021.05&#x2F;lib -lpython3.8”<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Bebop (Broadwell partition) with 36 MPI ranks per node. The <account_name> must be replaced with an active project allocation.</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –partition&#x3D;bdwall<br>#SBATCH –account&#x3D;<account_name><br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;36<br>#SBATCH –cpus-per-task&#x3D;1</p>
<p>module purge<br>module load intel-oneapi&#x2F;2021.4.0.3422<br>module load anaconda3&#x2F;2021.05</p>
<p>export LD_LIBRARY_PATH&#x3D;&#x2F;gpfs&#x2F;fs1&#x2F;home&#x2F;software&#x2F;anaconda3&#x2F;2021.05&#x2F;lib:$LD_LIBRARY_PATH<br>export OMP_NUM_THREADS&#x3D;1</p>
<p>ulimit -s unlimited</p>
<p>srun -n 2 -N 72 .&#x2F;wstat.x -i wstat.in &gt; wstat.out<br>To run on the KNL partition, use the following flags:</p>
<p>#SBATCH –partition&#x3D;knlall<br>#SBATCH –constraint knl,quad,cache<br>#SBATCH –ntasks-per-node&#x3D;64<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the LCRC user guide.</p>
<p>ANL-LCRC-Swing<br>Swing is an HPC cluster maintained by the Laboratory Computing Resource Center (LCRC) at Argonne National Laboratory.</p>
<p>$ ssh <username>@swing.lcrc.anl.gov<br>Building WEST<br>WEST executables can be compiled using the following script (tested on December 21, 2022):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load nvhpc&#x2F;21.9-4pt64om<br>export NVHPC_HOME&#x3D;&#x2F;gpfs&#x2F;fs1&#x2F;soft&#x2F;swing&#x2F;spack-0.16.1&#x2F;opt&#x2F;spack&#x2F;linux-ubuntu20.04-x86_64&#x2F;gcc-9.3.0&#x2F;nvhpc-21.9-4pt64om&#x2F;Linux_x86_64&#x2F;21.9<br>export LD_LIBRARY_PATH&#x3D;$NVHPC_HOME&#x2F;comm_libs&#x2F;openmpi4&#x2F;openmpi-4.0.5&#x2F;lib:$LD_LIBRARY_PATH<br>export PATH&#x3D;$NVHPC_HOME&#x2F;comm_libs&#x2F;openmpi4&#x2F;openmpi-4.0.5&#x2F;bin:$PATH<br>export SCALAPACK_LIBS&#x3D;$NVHPC_HOME&#x2F;comm_libs&#x2F;openmpi4&#x2F;openmpi-4.0.5&#x2F;lib&#x2F;libscalapack.a</p>
<p>.&#x2F;configure –with-cuda&#x3D;$NVHPC_HOME&#x2F;cuda&#x2F;11.0 –with-cuda-cc&#x3D;80 –with-cuda-runtime&#x3D;11.0</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”-L&#x2F;usr&#x2F;lib&#x2F;python3.8&#x2F;config-3.8-x86_64-linux-gnu -lpython3.8”<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on one node of Swing with 8 MPI ranks and 8 GPUs. The <account_name> must be replaced with an active project allocation.</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –account&#x3D;<account_name><br>#SBATCH –nodes&#x3D;1<br>#SBATCH –gres&#x3D;gpu:8</p>
<p>module load nvhpc&#x2F;21.9-4pt64om<br>export NVHPC_HOME&#x3D;&#x2F;gpfs&#x2F;fs1&#x2F;soft&#x2F;swing&#x2F;spack-0.16.1&#x2F;opt&#x2F;spack&#x2F;linux-ubuntu20.04-x86_64&#x2F;gcc-9.3.0&#x2F;nvhpc-21.9-4pt64om&#x2F;Linux_x86_64&#x2F;21.9<br>export LD_LIBRARY_PATH&#x3D;$NVHPC_HOME&#x2F;comm_libs&#x2F;openmpi4&#x2F;openmpi-4.0.5&#x2F;lib:$LD_LIBRARY_PATH<br>export PATH&#x3D;$NVHPC_HOME&#x2F;comm_libs&#x2F;openmpi4&#x2F;openmpi-4.0.5&#x2F;bin:$PATH</p>
<p>export OMP_NUM_THREADS&#x3D;1</p>
<p>mpirun -n 8 .&#x2F;wstat.x -i wstat.in &gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the LCRC user guide.</p>
<p>macOS<br>The following instructions have been tested on macOS 12.3 (with Intel chip).</p>
<p>Requirements:</p>
<p>C and Fortran compilers (e.g. gcc&#x2F;gfortran in GCC 11)</p>
<p>MPI (e.g. OpenMPI)</p>
<p>BLAS&#x2F;LAPACK</p>
<p>ScaLAPACK (optional)</p>
<p>FFTW3</p>
<p>Python3</p>
<p>Building WEST<br>WEST executables can be compiled using the following script:</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>export MY_LIB_PATH&#x3D;&#x2F;Users&#x2F;myname&#x2F;LIBRARIES</p>
<p>export CPP&#x3D;’cpp-11’<br>export CC&#x3D;’gcc-11’<br>export MPIF90&#x3D;’mpif90’<br>export F90&#x3D;’mpif90’<br>export BLAS_LIBS&#x3D;${MY_LIB_PATH}&#x2F;BLAS&#x2F;libblas.a<br>export LAPACK_LIBS&#x3D;${MY_LIB_PATH}&#x2F;LAPACK&#x2F;liblapack.a<br>export SCALAPACK_LIBS&#x3D;${MY_LIB_PATH}&#x2F;SCALAPACK&#x2F;libscalapack.a<br>export FFT_LIBS&#x3D;”${MY_LIB_PATH}&#x2F;FFTW3&#x2F;lib&#x2F;libfftw3.a ${MY_LIB_PATH}&#x2F;FFTW3&#x2F;lib&#x2F;libfftw3_omp.a”</p>
<p>.&#x2F;configure –enable-openmp –with-scalapack</p>
<p>make -j 4 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>make -j 4 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST<br>We can run the wstat.x WEST executables on 2 cores using the following command:</p>
<p>$ export OMP_NUM_THREADS&#x3D;1<br>$ mpirun -np 2 .&#x2F;wstat.x -i wstat.in &gt; wstat.out</p>
<p>NERSC-Cori<br>Cori (retired on May 31, 2023) was a Cray XC40 located at National Energy Research Scientific Computing Center (NERSC).</p>
<p>$ ssh <username>@cori.nersc.gov<br>Building WEST<br>WEST executables can be compiled using the following script:</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module unload cray-libsci<br>module load cray-python&#x2F;3.9.7.1</p>
<p>export CRAYPE_LINK_TYPE&#x3D;dynamic<br>export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;opt&#x2F;python&#x2F;3.9.7.1&#x2F;lib<br>export MPIF90&#x3D;ftn<br>export F90&#x3D;ftn<br>export CC&#x3D;cc<br>export SCALAPACK_LIBS&#x3D;”$MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_scalapack_lp64.so -Wl,–start-group $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_lp64.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_intel_thread.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_core.so $MKLROOT&#x2F;lib&#x2F;intel64&#x2F;libmkl_blacs_intelmpi_lp64.so -Wl,–end-group”</p>
<p>.&#x2F;configure –enable-openmp –with-scalapack&#x3D;intel</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Cori (Haswell partition) with 32 MPI ranks per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: The following environment variable is needed to work around a bug in ROMIO, Cray MPICH.</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”<br>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>#SBATCH –job-name&#x3D;WEST<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –account&#x3D;<project_name><br>#SBATCH –constraint&#x3D;haswell<br>#SBATCH –qos&#x3D;debug<br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;32<br>#SBATCH –cpus-per-task&#x3D;2</p>
<p>module unload cray-libsci<br>module load cray-python&#x2F;3.9.7.1</p>
<p>export CRAYPE_LINK_TYPE&#x3D;dynamic<br>export LD_LIBRARY_PATH&#x3D;$LD_LIBRARY_PATH:&#x2F;opt&#x2F;python&#x2F;3.9.7.1&#x2F;lib</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”</p>
<p>export OMP_NUM_THREADS&#x3D;1<br>export OMP_PLACE&#x3D;threads<br>export OMP_PROC_BIND&#x3D;spread<br>export MKL_NUM_THREADS&#x3D;$OMP_NUM_THREADS</p>
<p>NTASKS&#x3D;$(($SLURM_NTASKS_PER_NODE * $SLURM_JOB_NUM_NODES))</p>
<p>srun -N $SLURM_JOB_NUM_NODES -n $NTASKS -c $SLURM_CPUS_PER_TASK .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the NERSC user guide.</p>
<p>NERSC-Perlmutter<br>Perlmutter is an HPE Cray EX supercomputer located at National Energy Research Scientific Computing Center (NERSC). Perlmutter has both GPU-accelerated nodes and CPU-only nodes.</p>
<p>$ ssh <username>@saul-p1.nersc.gov<br>Building WEST (GPU)<br>WEST executables can be compiled using the following script (tested on May 20, 2023):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load PrgEnv-nvidia<br>module load nvidia&#x2F;22.7<br>module load cudatoolkit&#x2F;11.7<br>module load craype-accel-nvidia80<br>module load cray-python&#x2F;3.9.13.1</p>
<p>.&#x2F;configure –with-cuda&#x3D;$CUDA_HOME –with-cuda-runtime&#x3D;11.7 –with-cuda-cc&#x3D;80</p>
<p>#Manually edit make.inc:</p>
<p>#DFLAGS &#x3D; -D__PGI -D__CUDA -D__FFTW -D__MPI<br>#MPIF90 &#x3D; ftn<br>#F90 &#x3D; ftn<br>#CC &#x3D; cc<br>#LD &#x3D; ftn<br>#BLAS_LIBS &#x3D; #leave blank<br>#LAPACK_LIBS &#x3D; #leave blank</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs (GPU)<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two GPU nodes of Perlmutter with 4 MPI ranks and 4 GPUs per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: The following environment variable is needed to work around a bug in ROMIO, Cray MPICH.</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”<br>Important: It is recommended to run the calculation from the Lustre file system ($PSCRATCH instead of &#x2F;home).</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>#SBATCH –job-name&#x3D;WEST<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –account&#x3D;<project_name><br>#SBATCH –constraint&#x3D;gpu<br>#SBATCH –qos&#x3D;debug<br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;4<br>#SBATCH –gpus-per-node&#x3D;4<br>#SBATCH –cpus-per-task&#x3D;32</p>
<p>module load PrgEnv-nvidia<br>module load nvidia&#x2F;22.7<br>module load cudatoolkit&#x2F;11.7<br>module load craype-accel-nvidia80<br>module load cray-python&#x2F;3.9.13.1</p>
<p>export OMP_NUM_THREADS&#x3D;1<br>export SLURM_CPU_BIND&#x3D;cores<br>export MPICH_MPIIO_HINTS&#x3D;”*:romio_cb_write&#x3D;enable:romio_ds_write&#x3D;disable”<br>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”</p>
<p>srun -N 2 -n 8 -c 32 -G 8 .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>Building WEST (CPU)<br>WEST executables can be compiled using the following script (tested on May 20, 2023):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load cpu<br>module load cray-fftw&#x2F;3.3.10.3<br>module load cray-python&#x2F;3.9.13.1</p>
<p>export CRAYPE_LINK_TYPE&#x3D;dynamic<br>export MPIF90&#x3D;ftn<br>export F90&#x3D;ftn<br>export CC&#x3D;cc</p>
<p>.&#x2F;configure –enable-openmp –with-scalapack</p>
<p>#Manually edit make.inc:</p>
<p>#DFLAGS &#x3D; -D__FFTW3 -D__MPI -D__SCALAPACK<br>#IFLAGS &#x3D; -I. -I$(TOPDIR)&#x2F;include -I$(TOPDIR)&#x2F;FoX&#x2F;finclude -I&#x2F;opt&#x2F;cray&#x2F;pe&#x2F;fftw&#x2F;3.3.10.3&#x2F;x86_milan&#x2F;include<br>#BLAS_LIBS &#x3D; #leave blank<br>#LAPACK_LIBS &#x3D; #leave blank</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs (CPU)<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two CPU nodes of Perlmutter with 128 MPI ranks per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: The following environment variable is needed to work around a bug in ROMIO, Cray MPICH.</p>
<p>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”<br>Important: It is recommended to run the calculation from the Lustre file system ($PSCRATCH instead of &#x2F;home).</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>#SBATCH –job-name&#x3D;WEST<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –account&#x3D;<project_name><br>#SBATCH –constraint&#x3D;cpu<br>#SBATCH –qos&#x3D;debug<br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;128<br>#SBATCH –cpus-per-task&#x3D;2</p>
<p>module load cpu<br>module load cray-fftw&#x2F;3.3.10.3<br>module load cray-python&#x2F;3.9.13.1</p>
<p>export OMP_NUM_THREADS&#x3D;1<br>export SLURM_CPU_BIND&#x3D;cores<br>export MPICH_MPIIO_HINTS&#x3D;”*:romio_cb_write&#x3D;enable:romio_ds_write&#x3D;disable”<br>export ROMIO_FSTYPE_FORCE&#x3D;”ufs:”</p>
<p>srun -N 2 -n 256 -c 2 .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the NERSC user guide.</p>
<p>NVIDIA DGX A100<br>The following instructions have been tested on an NVIDIA DGX A100 machine.</p>
<p>Requirements:</p>
<p>NVIDIA HPC SDK (23.5)</p>
<p>Python3</p>
<p>To download and install NVIDIA HPC SDK, do:</p>
<p>$ wget <a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/hpc-sdk/23.5/nvhpc_2023_235_Linux_x86_64_cuda_multi.tar.gz">https://developer.download.nvidia.com/hpc-sdk/23.5/nvhpc_2023_235_Linux_x86_64_cuda_multi.tar.gz</a><br>$ tar xpzf nvhpc_2023_235_Linux_x86_64_cuda_multi.tar.gz<br>$ nvhpc_2023_235_Linux_x86_64_cuda_multi&#x2F;install<br>Building WEST<br>WEST executables can be compiled using the following script:</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>export LD_LIBRARY_PATH&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;compilers&#x2F;lib:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;comm_libs&#x2F;11.8&#x2F;openmpi4&#x2F;openmpi-4.1.5&#x2F;lib:$LD_LIBRARY_PATH<br>export LD_LIBRARY_PATH&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;cuda&#x2F;11.8&#x2F;targets&#x2F;x86_64-linux&#x2F;lib:$LD_LIBRARY_PATH<br>export PATH&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;compilers&#x2F;bin:$PATH<br>export PATH&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;comm_libs&#x2F;11.8&#x2F;openmpi4&#x2F;openmpi-4.1.5&#x2F;bin:$PATH<br>export SCALAPACK_LIBS&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;comm_libs&#x2F;11.8&#x2F;openmpi4&#x2F;openmpi-4.1.5&#x2F;lib&#x2F;libscalapack.a</p>
<p>.&#x2F;configure –with-cuda&#x3D;&#x2F;path&#x2F;to&#x2F;nvidia&#x2F;hpc_sdk&#x2F;Linux_x86_64&#x2F;23.5&#x2F;cuda&#x2F;11.8 –with-cuda-cc&#x3D;80 –with-cuda-runtime&#x3D;11.8</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”-L&#x2F;usr&#x2F;lib&#x2F;python3.8&#x2F;config-3.8-x86_64-linux-gnu -lpython3.8”<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST<br>We can run the wstat.x WEST executables on 2 GPUs using the following command:</p>
<p>$ export OMP_NUM_THREADS&#x3D;1<br>$ mpirun -np 2 .&#x2F;wstat.x -i wstat.in &gt; wstat.out</p>
<p>OLCF-Summit<br>Summit is a GPU-accelerated supercomputer located at Oak Ridge National Laboratory, maintained by OLCF.</p>
<p>$ ssh <username>@summit.olcf.ornl.gov<br>Building WEST<br>WEST executables can be compiled using the following script (tested on August 9, 2022):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load nvhpc&#x2F;21.9<br>module load cuda&#x2F;11.0.3<br>module load spectrum-mpi&#x2F;10.4.0.3-20210112<br>module load essl&#x2F;6.3.0<br>module load netlib-lapack&#x2F;3.9.1<br>module load python&#x2F;3.8-anaconda3<br>module load git&#x2F;2.36.1<br>module unload darshan-runtime</p>
<p>export BLAS_LIBS&#x3D;”$OLCF_ESSL_ROOT&#x2F;lib64&#x2F;libessl.so”<br>export LAPACK_LIBS&#x3D;”$OLCF_ESSL_ROOT&#x2F;lib64&#x2F;libessl.so $OLCF_NETLIB_LAPACK_ROOT&#x2F;lib64&#x2F;liblapack.a”</p>
<p>.&#x2F;configure –with-cuda&#x3D;$OLCF_CUDA_ROOT –with-cuda-runtime&#x3D;11.0 –with-cuda-cc&#x3D;70</p>
<p>#Manually edit make.inc: add -D__GPU_MPI to DFLAGS</p>
<p>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>sed -i ‘s&#x2F;-L.*&#x2F;-L/autofs/nccs-svm1_sw/summit/python/3.8/anaconda3/2020.07-rhel8/lib -lpython3.8&#x2F;‘ west_make.inc<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>Summit uses the jsrun job manager. The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Summit with 6 MPI ranks and 6 GPUs per node. The <project_name> must be replaced with an active project allocation.</p>
<p>Important: It is recommended to run the calculation from the IBM Spectrum Scale file system ($MEMBERWORK instead of &#x2F;home).</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#BSUB -P <project_name><br>#BSUB -W 0:20<br>#BSUB -nnodes 2<br>#BSUB -J jobname<br>#BSUB -o jobname.%J<br>#BSUB -N<br>#BSUB -q debug</p>
<p>module load nvhpc&#x2F;21.9<br>module load cuda&#x2F;11.0.3<br>module load spectrum-mpi&#x2F;10.4.0.3-20210112<br>module load essl&#x2F;6.3.0<br>module load netlib-lapack&#x2F;3.9.1<br>module load python&#x2F;3.8-anaconda3<br>module unload darshan-runtime</p>
<p>export OMP_NUM_THREADS&#x3D;1</p>
<p>#The following env vars improve MPI I&#x2F;O performance</p>
<p>export PAMI_ENABLE_STRIPING&#x3D;1<br>export PAMI_IBV_ADAPTER_AFFINITY&#x3D;1<br>export PAMI_IBV_DEVICE_NAME&#x3D;”mlx5_0:1,mlx5_3:1”<br>export PAMI_IBV_DEVICE_NAME_1&#x3D;”mlx5_3:1,mlx5_0:1”</p>
<p>export OMPI_MCA_io&#x3D;romio321<br>export ROMIO_HINTS&#x3D;&#x2F;path&#x2F;to&#x2F;romio_hints</p>
<p>jsrun -n 4 -a 3 -c 3 -g 3 -r 2 –smpiargs&#x3D;”-gpu” .&#x2F;wstat.x -i wstat.in &amp;&gt; wstat.out<br>The value of -n should be two times the number of nodes. When running QE and WEST, usually there is no need to change -a, -c, -g, and -r.</p>
<p>romio_hints is a text file with the following content:</p>
<p>romio_cb_write enable<br>romio_ds_write enable<br>cb_buffer_size 16777216<br>cb_nodes 2<br>Job submission is done with the following:</p>
<p>$ bsub run_west.sh<br>See also</p>
<p>For more information, visit the OLCF user guide.</p>
<p>hrj<br>qwer1234</p>
<p>UChicago-Midway2<br>Midway2 is the HPC cluster of the University of Chicago, maintained by UChicago’s RCC.</p>
<p>$ ssh <username>@midway2.rcc.uchicago.edu<br>Building WEST<br>WEST executables can be compiled using the following script (tested on June 23, 2022):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load intel&#x2F;19.1.1<br>module load intelmpi&#x2F;2019.up7+intel-19.1.1<br>module load mkl&#x2F;2020.up1<br>module load python&#x2F;cpython-3.8.5</p>
<p>export MPIF90&#x3D;mpiifort<br>export F90&#x3D;ifort<br>export CC&#x3D;icc<br>export SCALAPACK_LIBS&#x3D;”-lmkl_scalapack_lp64 -Wl,–start-group -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -Wl,–end-group”</p>
<p>.&#x2F;configure –with-scalapack&#x3D;intel –enable-openmp<br>make -j 8 pw</p>
<p>cd West<br>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>sed -i ‘s&#x2F;-L.*config-3.8-x86_64-linux-gnu &#x2F;&#x2F;‘ west_make.inc<br>make -j 8 all<br>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Midway2 with 32 MPI ranks per node. The <project_name> and <account_name> must be replaced with an active project allocation.</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –partition&#x3D;<partition_name><br>#SBATCH –account&#x3D;<account_name><br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;48<br>#SBATCH –cpus-per-task&#x3D;1</p>
<p>module load intel&#x2F;19.1.1<br>module load intelmpi&#x2F;2019.up7+intel-19.1.1<br>module load mkl&#x2F;2020.up1<br>module load python&#x2F;cpython-3.8.5</p>
<p>export I_MPI_PMI_LIBRARY&#x3D;&#x2F;software&#x2F;slurm-current-$DISTARCH&#x2F;lib&#x2F;libpmi.so<br>export LD_LIBRARY_PATH&#x3D;&#x2F;software&#x2F;python-3.8.5-el7-x86_64&#x2F;lib:$LD_LIBRARY_PATH<br>export OMP_NUM_THREADS&#x3D;1</p>
<p>srun -n 96 -N 2 .&#x2F;wstat.x -i wstat.in &gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the RCC user guide.</p>
<p>UChicago-Midway3<br>Midway3 is the HPC cluster of the University of Chicago, maintained by UChicago’s RCC.</p>
<p>$ ssh <username>@midway3.rcc.uchicago.edu<br>Building WEST<br>WEST executables can be compiled using the following script (tested on May 4, 2022):</p>
<p>$ cat build_west.sh<br>#!&#x2F;bin&#x2F;bash</p>
<p>module load intel&#x2F;19.1.1<br>module load intelmpi&#x2F;2019.up7+intel-19.1.1<br>module load mkl&#x2F;2020.up1<br>module load python&#x2F;anaconda-2020.11</p>
<p>export MPIF90&#x3D;mpiifort<br>export F90&#x3D;ifort<br>export CC&#x3D;icc<br>export SCALAPACK_LIBS&#x3D;”-lmkl_scalapack_lp64 -Wl,–start-group -lmkl_intel_lp64 -lmkl_intel_thread -lmkl_core -lmkl_blacs_intelmpi_lp64 -Wl,–end-group”</p>
<p>.&#x2F;configure –with-scalapack&#x3D;intel –enable-openmp<br>make -j 8 pw</p>
<p>cd West</p>
<p>make conf PYT&#x3D;python3 PYT_LDFLAGS&#x3D;”<code>python3-config --ldflags --embed</code>“<br>sed -i ‘s&#x2F;-L.*config-3.8-x86_64-linux-gnu &#x2F;&#x2F;‘ west_make.inc<br>make -j 8 all</p>
<p>To use the script do:</p>
<p>$ bash build_west.sh<br>Running WEST Jobs<br>The following is an example executable script run_west.sh to run the wstat.x WEST executable on two nodes of Midway3 with 32 MPI ranks per node. The <project_name> and <account_name> must be replaced with an active project allocation.</p>
<p>$ cat run_west.sh<br>#!&#x2F;bin&#x2F;bash<br>#SBATCH –time&#x3D;00:20:00<br>#SBATCH –partition&#x3D;<partition_name><br>#SBATCH –account&#x3D;<account_name><br>#SBATCH –nodes&#x3D;2<br>#SBATCH –ntasks-per-node&#x3D;48<br>#SBATCH –cpus-per-task&#x3D;1</p>
<p>module load intel&#x2F;19.1.1<br>module load intelmpi&#x2F;2019.up7+intel-19.1.1<br>module load mkl&#x2F;2020.up1<br>module load python&#x2F;anaconda-2020.11</p>
<p>export I_MPI_PMI_LIBRARY&#x3D;&#x2F;software&#x2F;slurm-current-$DISTARCH&#x2F;lib&#x2F;libpmi.so<br>export LD_LIBRARY_PATH&#x3D;&#x2F;software&#x2F;python-anaconda-2020.11-el8-x86_64&#x2F;lib:$LD_LIBRARY_PATH<br>export OMP_NUM_THREADS&#x3D;1</p>
<p>srun -n 96 -N 2 .&#x2F;wstat.x -i wstat.in &gt; wstat.out<br>Job submission is done with the following:</p>
<p>$ sbatch run_west.sh<br>See also</p>
<p>For more information, visit the RCC user guide.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/15/llm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/15/llm/" class="post-title-link" itemprop="url">llm</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-15 22:55:06" itemprop="dateCreated datePublished" datetime="2024-06-15T22:55:06+08:00">2024-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="chatglm"><a href="#chatglm" class="headerlink" title="chatglm"></a>chatglm</h1><p>git clone <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">https://github.com/THUDM/ChatGLM-6B</a><br>curl -s <a target="_blank" rel="noopener" href="https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh">https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh</a> | sudo bash<br>sudo apt-get install git-lfs</p>
<p>在ChatGLM-6B 目录里面创建一个model的文件夹<br>git clone <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm-6b-int4">https://huggingface.co/THUDM/chatglm-6b-int4</a></p>
<p>pip install -r requirements.txt<br>pip install gradio&#x3D;&#x3D;3.50.0</p>
<p>执行 vim web_demo.py 修改文档目录中的 web_demo.py文件,修改其中的两处代码中的路径，将原始的“THUDM&#x2F;ChatGLM-6B”替为当前的模型路径”model&#x2F;chatglm-6b-int4“，：<br>tokenizer &#x3D; AutoTokenizer.from_pretrained(“model&#x2F;chatglm-6b-int4”, trust_remote_code&#x3D;True)<br>model &#x3D; AutoModel.from_pretrained(“model&#x2F;chatglm-6b-int4”, trust_remote_code&#x3D;True).half().cuda()</p>
<p>7、修改web_demo.py文件中最后，启动文件时，产生公共链接的参数，将share&#x3D;False,改为share&#x3D;True</p>
<p>demo.queue().launch(share&#x3D;True, inbrowser&#x3D;True)<br>8、安装cudatoolkit，执行</p>
<p>conda install cudatoolkit&#x3D;11.7 -c nvidia<br>conda install cudatoolkit -c nvidia<br>如不成功，则更换安装源，执行</p>
<p>conda install -c conda-forge cudatoolkit</p>
<p>执行python web_demo.py，启动服务<br>可以在本地电脑浏览器上用http:&#x2F;&#x2F;你服务器的ip:7860上查看，也可以点击生成的公用URL查看效果</p>
<p>Could not create share link. Missing file: &#x2F;opt&#x2F;conda&#x2F;envs&#x2F;chatglm&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;gradio&#x2F;frpc_linux_amd64_v0.2. </p>
<p>Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: </p>
<ol>
<li>Download this file: <a target="_blank" rel="noopener" href="https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64">https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64</a></li>
<li>Rename the downloaded file to: frpc_linux_amd64_v0.2</li>
<li>Move the file to this location: &#x2F;opt&#x2F;conda&#x2F;envs&#x2F;chatglm&#x2F;lib&#x2F;python3.11&#x2F;site-packages&#x2F;gradio</li>
</ol>
<p>curl -O <a target="_blank" rel="noopener" href="https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64">https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64</a></p>
<p>chmod +x frpc_linux_amd64_v0.2</p>
<p>python -c “import torch; print(torch.cuda.is_available())”</p>
<p>Maximum length 参数</p>
<p>通常用于限制输入序列的最大长度，因为 ChatGLM-6B 是 2048 长度推理的，一般这个保持默认就行，太大可能会导致性能下降。</p>
<p>Top P 参数</p>
<p>Top P 参数是指在生成文本等任务中，选择可能性最高的前 P 个词的概率累加和。这个参数被称为 Top P，也称为 Nucleus Sampling。</p>
<p>例如，如果将 Top P 参数设置为 0.7，那么模型会选择可能性排名超过 70% 的词进行采样。这样可以保证生成的文本准确性较高，但可能会缺之多样性。相反，如果将 Top P 参教设置为 0.3，则会选择可能性超过 30% 的词进行采样，这可能会导致生成义本的准确性下降，但能够更好地增加多样性。</p>
<p>Temperature 参数</p>
<p>Temperature 参数通常用于调整 softmax 函数的输出，用于增加或减少模型对不类别的置信度。具体来说，softmax 函数将模型对每个类别的预测转换为概率分布。Temperature 参数可以看作是一个缩放因子，它可以增加或减少 softmax 函数输出中每个类别的置信度。</p>
<p>比如将 Temperature 设置为 0.05 和 0.95 的主要区别在于，T&#x3D;0.05 会使得模型更加自信，更加倾向于选择概率最大的类别作为输出，而 T&#x3D;0.95 会使得模型更加不确定，更加倾向于输出多个类别的概率值较大。</p>
<h1 id="chatglm-1"><a href="#chatglm-1" class="headerlink" title="chatglm"></a>chatglm</h1><p><a target="_blank" rel="noopener" href="https://huggingface.co/THUDM">https://huggingface.co/THUDM</a><br><a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B">https://github.com/THUDM/ChatGLM-6B</a><br><a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv22991124/">https://www.bilibili.com/read/cv22991124/</a></p>
<h1 id="多卡推理"><a href="#多卡推理" class="headerlink" title="多卡推理"></a>多卡推理</h1><p><a target="_blank" rel="noopener" href="https://github.com/jia-zhuang/pytorch-multi-gpu-training">https://github.com/jia-zhuang/pytorch-multi-gpu-training</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/BIT_666/article/details/132538581">https://blog.csdn.net/BIT_666/article/details/132538581</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2365232">https://cloud.tencent.com/developer/article/2365232</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/baidu_38270845/article/details/105823344">https://blog.csdn.net/baidu_38270845/article/details/105823344</a><br><a target="_blank" rel="noopener" href="https://github.com/ChuangLee/ChatGLM-6B-multiGPU">https://github.com/ChuangLee/ChatGLM-6B-multiGPU</a></p>
<h1 id="门的耳朵"><a href="#门的耳朵" class="headerlink" title="门的耳朵"></a>门的耳朵</h1><p><a target="_blank" rel="noopener" href="https://space.bilibili.com/508414342?spm_id_from=333.788.0.0">https://space.bilibili.com/508414342?spm_id_from=333.788.0.0</a></p>
<h1 id="glm4"><a href="#glm4" class="headerlink" title="glm4"></a>glm4</h1><p>GLM-4-9B 是智谱 AI 推出的最新一代预训练模型 GLM-4 系列中的开源版本。 在语义、数学、推理、代码和知识等多方面的数据集测评中， GLM-4-9B 及其人类偏好对齐的版本 GLM-4-9B-Chat 均表现出超越 Llama-3-8B 的卓越性能。除了能进行多轮对话，GLM-4-9B-Chat 还具备网页浏览、代码执行、自定义工具调用（Function Call）和长文本推理（支持最大 128K 上下文）等高级功能。本代模型增加了多语言支持，支持包括日语，韩语，德语在内的 26 种语言。我们还推出了支持 1M 上下文长度（约 200 万中文字符）的 GLM-4-9B-Chat-1M 模型和基于 GLM-4-9B 的多模态模型 GLM-4V-9B。GLM-4V-9B 具备 1120 * 1120 高分辨率下的中英双语多轮对话能力，在中英文综合能力、感知推理、文字识别、图表理解等多方面多模态评测中，GLM-4V-9B 表现出超越 GPT-4-turbo-2024-04-09、Gemini 1.0 Pro、Qwen-VL-Max 和 Claude 3 Opus 的卓越性能。</p>
<p>install conda pip git-lfs</p>
<p>git lfs install<br>git clone <a target="_blank" rel="noopener" href="https://www.modelscope.cn/ZhipuAI/glm-4-9b.git">https://www.modelscope.cn/ZhipuAI/glm-4-9b.git</a><br>git clone <a target="_blank" rel="noopener" href="https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat-1m.git">https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat-1m.git</a><br>git clone <a target="_blank" rel="noopener" href="https://www.modelscope.cn/ZhipuAI/glm-4v-9b.git">https://www.modelscope.cn/ZhipuAI/glm-4v-9b.git</a><br>git clone <a target="_blank" rel="noopener" href="https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat.git">https://www.modelscope.cn/ZhipuAI/glm-4-9b-chat.git</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4">https://github.com/THUDM/GLM-4</a><br>git clone <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM-4.git">https://github.com/THUDM/GLM-4.git</a></p>
<p>ref <a target="_blank" rel="noopener" href="https://github.com/datawhalechina/self-llm">https://github.com/datawhalechina/self-llm</a></p>
<!-- #MODEL_PATH = os.environ.get('MODEL_PATH', 'THUDM/glm-4-9b-chat')
MODEL_PATH = os.environ.get('MODEL_PATH', '/home/duguex/glm4/glm-4-9b-chat')
pip install tiktoken
pip install accelerate -->

<p>最低硬件要求<br>如果您希望运行官方提供的最基础代码 (transformers 后端) 您需要：</p>
<p>Python &gt;&#x3D; 3.10<br>内存不少于 32 GB<br>如果您希望运行官方提供的本文件夹的所有代码，您还需要：</p>
<p>Linux 操作系统 (Debian 系列最佳)<br>大于 8GB 显存的，支持 CUDA 或者 ROCM 并且支持 BF16 推理的 GPU 设备。(FP16 精度无法训练，推理有小概率出现问题)<br>安装依赖</p>
<p>pip install -r requirements.txt<br>基础功能调用<br>除非特殊说明，本文件夹所有 demo 并不支持 Function Call 和 All Tools 等进阶用法</p>
<p>使用 transformers 后端代码<br>使用命令行与 GLM-4-9B 模型进行对话。<br>python trans_cli_demo.py # GLM-4-9B-Chat<br>python trans_cli_vision_demo.py # GLM-4V-9B<br>使用 Gradio 网页端与 GLM-4-9B-Chat 模型进行对话。<br>python trans_web_demo.py<br>使用 Batch 推理。<br>python cli_batch_request_demo.py<br>使用 vLLM 后端代码<br>使用命令行与 GLM-4-9B-Chat 模型进行对话。<br>python vllm_cli_demo.py<br>自行构建服务端，并使用 OpenAI API 的请求格式与 GLM-4-9B-Chat 模型进行对话。本 demo 支持 Function Call 和 All Tools功能。<br>启动服务端：</p>
<p>python openai_api_server.py<br>客户端请求：</p>
<p>python openai_api_request.py<br>压力测试<br>用户可以在自己的设备上使用本代码测试模型在 transformers后端的生成速度:</p>
<p>python trans_stress_test.py</p>
<h1 id="use-vllm"><a href="#use-vllm" class="headerlink" title="use vllm"></a>use vllm</h1><h1 id="vllm-0-4-3"><a href="#vllm-0-4-3" class="headerlink" title="vllm&gt;&#x3D;0.4.3"></a>vllm&gt;&#x3D;0.4.3</h1><p>vllm 对ray最低版本有要求,而ray没有支持最新的python版本，python&#x3D;3.11.9 instead of python12</p>
<p>AssertionError: CUDA_HOME is not set<br>需要安装cuda，cuda版本与显卡驱动版本一致<br>这个错误信息表明，你的环境变量 <code>CUDA_HOME</code> 没有被设置。<code>CUDA_HOME</code> 是一个环境变量，通常用于指向 CUDA 安装的位置。CUDA 是 NVIDIA 开发的一个平台，用于进行高性能的并行计算。</p>
<p>你可以通过以下步骤来设置 <code>CUDA_HOME</code>：</p>
<ol>
<li><p>找到 CUDA 的安装位置。在许多 Linux 系统上，CUDA 通常安装在 <code>/usr/local/cuda</code>。</p>
</li>
<li><p>打开终端，运行以下命令来设置 <code>CUDA_HOME</code> 环境变量（假设 CUDA 安装在 <code>/usr/local/cuda</code>）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure>
</li>
<li><p>为了让这个设置在每次打开新的终端时都生效，你可以将上面的 <code>export</code> 命令添加到你的 shell 配置文件中，例如 <code>~/.bashrc</code> 或 <code>~/.zshrc</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;export CUDA_HOME=/usr/local/cuda&#x27;</span> &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>然后，运行 <code>source ~/.bashrc</code> 来使改动立即生效。</p>
</li>
</ol>
<p>请注意，你需要根据你的 CUDA 实际的安装位置来设置 <code>CUDA_HOME</code>。如果你不确定 CUDA 的安装位置，你可以尝试运行 <code>which nvcc</code> 命令，这个命令会显示 <code>nvcc</code>（CUDA 的编译器）的路径，这个路径通常可以用来确定 CUDA 的安装位置。目录不包含&#x2F;bin</p>
<p>如果你通过 <code>pip</code> 安装了 CUDA 相关的 Python 包（如 <code>torch</code> 或 <code>cupy</code>），那么 CUDA 库本身通常会被安装在这些 Python 包的目录中，而不是在系统级别的目录（如 <code>/usr/local/cuda</code>）中。</p>
<p>export CUDA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda</p>
<p>wget <a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb</a><br>sudo dpkg -i cuda-keyring_1.1-1_all.deb<br>sudo apt-get update<br>sudo apt-get -y install cuda</p>
<p>wget <a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin">https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin</a><br>sudo mv cuda-ubuntu2204.pin &#x2F;etc&#x2F;apt&#x2F;preferences.d&#x2F;cuda-repository-pin-600<br>wget <a target="_blank" rel="noopener" href="https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb">https://developer.download.nvidia.com/compute/cuda/12.2.2/local_installers/cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb</a><br>sudo dpkg -i cuda-repo-ubuntu2204-12-2-local_12.2.2-535.104.05-1_amd64.deb<br>sudo cp &#x2F;var&#x2F;cuda-repo-ubuntu2204-12-2-local&#x2F;cuda-*-keyring.gpg &#x2F;usr&#x2F;share&#x2F;keyrings&#x2F;<br>sudo apt-get update<br>sudo apt-get install cuda</p>
<p>如果你想撤销 <code>sudo dpkg -i cuda-keyring_1.1-1_all.deb</code> 这个操作，你可以使用 <code>dpkg -r</code> 或 <code>dpkg --remove</code> 命令来移除 <code>cuda-keyring</code> 包。以下是具体的命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -r cuda-keyring</span><br></pre></td></tr></table></figure>

<p>这个命令会移除 <code>cuda-keyring</code> 包，但它不会移除该包的配置文件。如果你想完全移除该包，包括其配置文件，你可以使用 <code>dpkg -P</code> 或 <code>dpkg --purge</code> 命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -P cuda-keyring</span><br></pre></td></tr></table></figure>

<p>请注意，这些命令只会移除 <code>cuda-keyring</code> 包，它们不会移除该包的依赖或者该包安装的其他文件。如果你需要完全移除 CUDA，你可能需要使用其他的方法，例如使用 NVIDIA 提供的卸载脚本。</p>
<h1 id="vllm-is-supporting-P100-now"><a href="#vllm-is-supporting-P100-now" class="headerlink" title="vllm is supporting P100 now"></a>vllm is supporting P100 now</h1><p>need change dtype from bfloat16 to float16</p>
<p>ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla P100-PCIE-16GB GPU has compute capability 6.0. You can use float16 instead by explicitly setting the<code>dtype</code> flag in CLI, for example: –dtype&#x3D;half.</p>
<h1 id="ollama"><a href="#ollama" class="headerlink" title="ollama"></a>ollama</h1><p>ollama run llama3:70b<br>ollama basepath</p>
<h1 id="Claude-3-5-Sonnet-GPT4o"><a href="#Claude-3-5-Sonnet-GPT4o" class="headerlink" title="Claude 3.5 Sonnet - GPT4o"></a>Claude 3.5 Sonnet - GPT4o</h1><p>ollama + chatbox &#x2F;open-webui(多模态)<br>huggingface(gguf) + chatbox<br>api调用是什么</p>
<p>obsidian 第三方插件 text generator&#x2F; copilot<br>default prompts package</p>
<p>LM Studio (model scope) v.s. ollama v.s. anythingLLMdesktop</p>
<p>LLM preferences qwen2 in LM Studio<br>embedder preferences acge in ollama</p>
<p>微调 unsloth v.s. llama-factory<br>微调 lora</p>
<p>ollama 是 lama.cpp的套壳 对大并发的支持不好<br>vllm 用于大并发 </p>
<p>“registry-mirrors”:[“<a target="_blank" rel="noopener" href="https://docker.m.daocloud.io","https//docker.1panel.live%22]">https://docker.m.daocloud.io&quot;,&quot;https://docker.1panel.live&quot;]</a></p>
<p>chatollama:<br>redis-1        | 1:C 02 Jul 2024 01:57:03.863 # WARNING Memory overcommit must be enabled! Without it, a background save or replication may fail under low memory condition. Being disabled, it can also cause failures without low memory condition, see <a target="_blank" rel="noopener" href="https://github.com/jemalloc/jemalloc/issues/1328">https://github.com/jemalloc/jemalloc/issues/1328</a>. </p>
<p>To fix this issue add ‘vm.overcommit_memory &#x3D; 1’ to &#x2F;etc&#x2F;sysctl.conf and then reboot or run the command ‘sysctl vm.overcommit_memory&#x3D;1’ for this to take effect.</p>
<h1 id="chatollama-ollama"><a href="#chatollama-ollama" class="headerlink" title="chatollama+ ollama"></a>chatollama+ ollama</h1><p>chatollama in docker:</p>
<p>docker-compose.yml</p>
<p>services:<br>  chromadb:<br>    image: chromadb&#x2F;chroma<br>    ports:<br>      - “8000:8000”<br>    restart: always<br>    volumes:<br>      - chromadb_volume:&#x2F;chroma&#x2F;chroma</p>
<p>  chatollama:<br>    environment:<br>      - CHROMADB_URL&#x3D;<a target="_blank" rel="noopener" href="http://chromadb:8000/">http://chromadb:8000</a><br>      - DATABASE_URL&#x3D;file:&#x2F;app&#x2F;sqlite&#x2F;chatollama.sqlite<br>      - REDIS_HOST&#x3D;redis<br>      - COHERE_API_KEY&#x3D;xxxxx<br>      - COHERE_MODEL&#x3D;ms-marco-MiniLM-L-6-v2<br>      - COHERE_BASE_URL&#x3D;<a target="_blank" rel="noopener" href="http://peanutshell:8000/v1">http://peanutshell:8000/v1</a><br>    image: 0001coder&#x2F;chatollama:latest<br>    pull_policy: always<br>    #extra_hosts:<br>    #  - “host.docker.internal:host-gateway”<br>    ports:<br>      - “3000:3000”<br>    restart: always<br>    volumes:<br>      - ~&#x2F;.chatollama:&#x2F;app&#x2F;sqlite</p>
<p>  redis:<br>    image: redis:latest<br>    restart: always<br>    volumes:<br>      - redis_data:&#x2F;data</p>
<p>  peanutshell:<br>    image: ghcr.io&#x2F;sugarforever&#x2F;peanut-shell:latest<br>    volumes:<br>      - hf_data:&#x2F;root&#x2F;.cache</p>
<p>volumes:<br>  chromadb_volume:<br>  redis_data:<br>  hf_data:</p>
<p>ollama in host</p>
<p>Setting environment variables on Linux<br>If Ollama is run as a systemd service, environment variables should be set using systemctl:</p>
<p>Edit the systemd service by calling systemctl edit ollama.service. This will open an editor.</p>
<p>For each environment variable, add a line Environment under section [Service]:</p>
<p>[Service]<br>Environment&#x3D;”OLLAMA_HOST&#x3D;0.0.0.0”<br>Save and exit.</p>
<p>Reload systemd and restart Ollama:</p>
<p>systemctl daemon-reload<br>systemctl restart ollama</p>
<p>then change ollama host ip in chatollama website to host ip (192.168.1.114:11434)</p>
<p>add HF_ENDPOINT&#x3D;<a target="_blank" rel="noopener" href="https://hf-mirror.com/">https://hf-mirror.com</a> to docker-compose.yml<br>i dont know which service, so i add it to all services<br>to solve “We couldn’t connect to ‘<a target="_blank" rel="noopener" href="https://huggingface.co/">https://huggingface.co</a>‘ to load this file “<br>maybe due to langchain </p>
<p>services:<br>  chromadb:<br>    environment:<br>      - HF_ENDPOINT&#x3D;<a target="_blank" rel="noopener" href="https://hf-mirror.com/">https://hf-mirror.com</a><br>    image: chromadb&#x2F;chroma<br>    ports:<br>      - “8000:8000”<br>    restart: always<br>    volumes:<br>      - chromadb_volume:&#x2F;chroma&#x2F;chroma</p>
<p>  chatollama:<br>    environment:<br>      - CHROMADB_URL&#x3D;<a target="_blank" rel="noopener" href="http://chromadb:8000/">http://chromadb:8000</a><br>      - DATABASE_URL&#x3D;file:&#x2F;app&#x2F;sqlite&#x2F;chatollama.sqlite<br>      - REDIS_HOST&#x3D;redis<br>      - COHERE_API_KEY&#x3D;xxxxx<br>      - COHERE_MODEL&#x3D;ms-marco-MiniLM-L-6-v2<br>      - COHERE_BASE_URL&#x3D;<a target="_blank" rel="noopener" href="http://peanutshell:8000/v1">http://peanutshell:8000/v1</a><br>      - HF_ENDPOINT&#x3D;<a target="_blank" rel="noopener" href="https://hf-mirror.com/">https://hf-mirror.com</a><br>    image: 0001coder&#x2F;chatollama:latest<br>    pull_policy: always<br>    #extra_hosts:<br>    #  - “host.docker.internal:host-gateway”<br>    ports:<br>      - “3000:3000”<br>    restart: always<br>    volumes:<br>      - ~&#x2F;.chatollama:&#x2F;app&#x2F;sqlite</p>
<p>  redis:<br>    environment:<br>      - HF_ENDPOINT&#x3D;<a target="_blank" rel="noopener" href="https://hf-mirror.com/">https://hf-mirror.com</a><br>    image: redis:latest<br>    restart: always<br>    volumes:<br>      - redis_data:&#x2F;data</p>
<p>  peanutshell:<br>    environment:<br>      - HF_ENDPOINT&#x3D;<a target="_blank" rel="noopener" href="https://hf-mirror.com/">https://hf-mirror.com</a><br>    image: ghcr.io&#x2F;sugarforever&#x2F;peanut-shell:latest<br>    volumes:<br>      - hf_data:&#x2F;root&#x2F;.cache</p>
<p>volumes:<br>  chromadb_volume:<br>  redis_data:<br>  hf_data:</p>
<p>the file in the knowledge base should not be too large (40M), otherwise it will fail to load</p>
<p>due to chroma?<br>[nuxt] [request error] [unhandled] [500] Invalid string length</p>
<p><a target="_blank" rel="noopener" href="https://techdiylife.github.io/blog/blog.html?category1=c01&blogid=0060">https://techdiylife.github.io/blog/blog.html?category1=c01&amp;blogid=0060</a><br><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/linux.md">https://github.com/ollama/ollama/blob/main/docs/linux.md</a><br>目录索引</p>
<p>升级Ollama并试用 Gemma2<br>Ollama安装<br>步骤一：下载模型<br>步骤二：升级Ollama<br>步骤三：运行Gemma2模型<br>试用Gemma2共学问题<br>升级Ollama并试用 Gemma2<br>Ollama安装<br>如果你没有安装ollama，请参考下面的文档进行安装：<br>ollama应用全面解析：20个问题精通ollama</p>
<p>步骤一：下载模型<br>$ ollama run gemma2 # 9B模型<br>$ ollama run gemma2:27b # 7B模型</p>
<p>alt text<br>Error: exception error loading model architecture: unknown model architecture: ‘gemma2’</p>
<p>步骤二：升级Ollama<br>官方参考文档：<a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/blob/main/docs/linux.md">https://github.com/ollama/ollama/blob/main/docs/linux.md</a><br>install安装脚本操作步骤说明：<br><a target="_blank" rel="noopener" href="https://techdiylife.github.io/blog/blog.html?category1=c02&blogid=0036">https://techdiylife.github.io/blog/blog.html?category1=c02&amp;blogid=0036</a></p>
<p>确认Ollama版本，最新为 0.1.48 （截至2024.07.01）<br>$ ollama -v<br>ollama version is 0.1.29<br>下载ollama<br>$ curl -L <a target="_blank" rel="noopener" href="https://ollama.com/download/ollama-linux-amd64">https://ollama.com/download/ollama-linux-amd64</a> -o ollama<br>alt text</p>
<p>确认当前ollama的安装位置</p>
<p>$ which ollama<br>&#x2F;usr&#x2F;local&#x2F;bin&#x2F;ollama<br>替换安装文件</p>
<p>$ chmod +x ollama<br>$ sudo mv ollama &#x2F;usr&#x2F;sbin&#x2F;bin&#x2F;<br>重启服务</p>
<p>$ sudo systemctl restart ollama<br>如果不重启服务，将会看到如下提示：</p>
<p>$ ollama -v<br>ollama version is 0.1.29<br>Warning: client version is 0.1.48<br>步骤三：运行Gemma2模型<br>ollama run gemma2<br>试用Gemma2共学问题<br>任务1：在本地运行Gemma2模型，9B或者27B （使用Ollama，或者你熟悉的任何本地模型部署，运行工具）<br>任务2：问几个你感兴趣的问题，并与Qwen2，Llama3，GLM4-9b等同规模模型进行对比<br>任务3：了解Gemma2的性能如何？ 尤其是与Qwen2，GLM4相比性能如何？<br>任务4：了解Gemma2模型的主要改进有哪些？ 使用了什么技术，训练数据，网络结构有哪些变化？</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/15/misc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/15/misc/" class="post-title-link" itemprop="url">misc</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-15 22:01:46" itemprop="dateCreated datePublished" datetime="2024-06-15T22:01:46+08:00">2024-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="marp-markdown"><a href="#marp-markdown" class="headerlink" title="marp markdown"></a>marp markdown</h1><p>convert md to ppt&#x2F;slides<br><a target="_blank" rel="noopener" href="https://github.com/marp-team/marp-core#math-global-directive">https://github.com/marp-team/marp-core#math-global-directive</a></p>
<h1 id="abinit-compile"><a href="#abinit-compile" class="headerlink" title="abinit compile"></a>abinit compile</h1><p>apt install gfortran git libblas-dev libfftw3-dev libhdf5-dev liblapack-dev libnetcdf-dev libnetcdff-dev libopenmpi-dev libpnetcdf-dev libxc-dev libxml2-dev lrzsz make mpi-default-dev python3 unzip vim</p>
<h1 id="fortran-ignore-mismatch"><a href="#fortran-ignore-mismatch" class="headerlink" title="fortran ignore mismatch"></a>fortran ignore mismatch</h1><p>FCOPTS +&#x3D; -fallow-argument-mismatch</p>
<h2 id="Mouse-Without-Borders"><a href="#Mouse-Without-Borders" class="headerlink" title="Mouse Without Borders"></a>Mouse Without Borders</h2><p>你可以使用名为 “Mouse Without Borders” 的工具来实现这个功能。这是一个由 Microsoft Garage 项目开发的免费工具，可以让你使用一套键盘和鼠标来控制多达四台 Windows 计算机。</p>
<p>以下是使用 Mouse Without Borders 的步骤：</p>
<ol>
<li><p>在你的两台 Windows 11 计算机上下载并安装 Mouse Without Borders。你可以从 Microsoft Download Center 获取这个工具。</p>
</li>
<li><p>在第一台计算机上启动 Mouse Without Borders，然后点击 “No” 按钮来获取安全代码和计算机名。</p>
</li>
<li><p>在第二台计算机上启动 Mouse Without Borders，然后点击 “Yes” 按钮，输入第一台计算机的安全代码和计算机名。</p>
</li>
<li><p>点击 “Link” 按钮，然后两台计算机就会被连接起来，你可以使用一套键盘和鼠标来控制它们了。</p>
</li>
</ol>
<p>请注意，你的两台计算机需要在同一个局域网内，才能使用 Mouse Without Borders。</p>
<h1 id="JAX"><a href="#JAX" class="headerlink" title="JAX"></a>JAX</h1><p>NVIDIA GPU on x86_64	pip install -U “jax[cuda12_pip]” -f <a target="_blank" rel="noopener" href="https://storage.googleapis.com/jax-releases/jax_cuda_releases.html">https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</a></p>
<h1 id="libnvidia-ml"><a href="#libnvidia-ml" class="headerlink" title="libnvidia-ml"></a>libnvidia-ml</h1><p>NVIDIA GPU Support Autodetection of NVIDIA GPUs will be available if the libnvidia-ml development library is installed.</p>
<p><code>libnvidia-ml</code> 是 NVIDIA 管理库（NVIDIA Management Library，NVML）的一部分，它是一个 C 语言的 API，用于直接获取 NVIDIA GPU 的状态和管理&#x2F;监控工作。这个库可以让你获取关于 GPU 的各种信息，如温度、内存使用情况、运行的进程等。</p>
<p>在 Linux 系统中，你可以使用 <code>ldconfig</code> 命令来检查 <code>libnvidia-ml</code> 是否已经安装。在终端中输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ldconfig -p | grep libnvidia-ml</span><br></pre></td></tr></table></figure>

<p>如果 <code>libnvidia-ml</code> 已经安装，这个命令将输出一些包含 <code>libnvidia-ml</code> 的行。如果没有输出，那么 <code>libnvidia-ml</code> 没有安装。</p>
<p>在 Windows 系统中，<code>libnvidia-ml</code> 通常作为 NVIDIA 驱动程序的一部分自动安装。你可以在设备管理器中检查 NVIDIA GPU 驱动程序是否已经安装。</p>
<p>注意，使用 NVML 需要具有适当的系统权限。在某些系统中，你可能需要 root 权限或者在 NVIDIA 控制面板中设置适当的权限。</p>
<h1 id="win11-amd-cpu-tune"><a href="#win11-amd-cpu-tune" class="headerlink" title="win11 amd cpu tune"></a>win11 amd cpu tune</h1><p>1.CMD运行两条命令，执行完不会有任何提示，执行了就可以了<br>①powercfg -attributes SUB_PROCESSOR 93b8b6dc-0698-4d1c-9ee4-0644e900c85d -ATTRIB_HIDE<br>②powercfg -attributes SUB_PROCESSOR bae08b81-2d5e-4688-ad6a-13243356654b -ATTRIB_HIDE<br>2.选择电源计划，高性能或者卓越性能都可以<br>3.更改计划设置—更改高级电源设置—处理器电源管理<br>异类线程调度策略 和 异类短运行线程调度策略<br>这两个都改成所有处理器，再应用确定就可以了</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/15/gpaw/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/15/gpaw/" class="post-title-link" itemprop="url">gpaw</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-15 21:50:10" itemprop="dateCreated datePublished" datetime="2024-06-15T21:50:10+08:00">2024-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="GPAW-install"><a href="#GPAW-install" class="headerlink" title="GPAW install"></a>GPAW install</h1><p>conda install -c conda-forge gpaw pytest pytest-xdist</p>
<p>For Linux 64, Open MPI is built with CUDA awareness but this support is disabled by default.<br>To enable it, please set the environment variable OMPI_MCA_opal_cuda_support&#x3D;true before<br>launching your MPI processes. Equivalently, you can set the MCA parameter in the command line:<br>mpiexec –mca opal_cuda_support 1 …</p>
<p>In addition, the UCX support is also built but disabled by default.<br>To enable it, first install UCX (conda install -c conda-forge ucx). Then, set the environment<br>variables OMPI_MCA_pml&#x3D;”ucx” OMPI_MCA_osc&#x3D;”ucx” before launching your MPI processes.<br>Equivalently, you can set the MCA parameters in the command line:<br>mpiexec –mca pml ucx –mca osc ucx …<br>Note that you might also need to set UCX_MEMTYPE_CACHE&#x3D;n for CUDA awareness via UCX.<br>Please consult UCX’s documentation for detail.</p>
<p>collecting … ————————————————————————–<br>WARNING: Linux kernel CMA support was requested via the<br>btl_vader_single_copy_mechanism MCA variable, but CMA support is<br>not available due to restrictive ptrace settings.</p>
<p>The vader shared memory BTL will fall back on another single-copy<br>mechanism if one is available. This may result in lower performance.</p>
<h2 id="Local-host-duguex-NF5588M3"><a href="#Local-host-duguex-NF5588M3" class="headerlink" title="  Local host: duguex-NF5588M3"></a>  Local host: duguex-NF5588M3</h2><p>这个警告信息是由于 Open MPI 的 Vader BTL（Byte Transfer Layer）试图使用 Linux 的跨内存区（CMA）来进行高效的内存复制，但是由于系统的 ptrace 设置，CMA 不可用。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/microsoft/WSL/issues/3397">https://github.com/microsoft/WSL/issues/3397</a></p>
<p>你可以通过以下两种方法来解决这个问题：</p>
<ol>
<li><p>改变系统的 ptrace 设置。你可以通过运行以下命令来允许非 root 用户使用 CMA：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 | sudo <span class="built_in">tee</span> /proc/sys/kernel/yama/ptrace_scope</span><br></pre></td></tr></table></figure>

</li>
<li><p>禁用 Vader BTL 的 CMA 支持。你可以通过设置 <code>btl_vader_single_copy_mechanism</code> MCA 参数来禁用 CMA 支持：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> OMPI_MCA_btl_vader_single_copy_mechanism=none</span><br></pre></td></tr></table></figure>

<p> 这个设置会使 Vader BTL 使用其他的内存复制机制，可能会导致性能降低。</p>
</li>
</ol>
<p>在选择解决方案时，你需要根据你的具体情况和需求来决定。如果你的应用对性能要求很高，你可能需要改变系统的 ptrace 设置。如果你的应用对性能要求不高，或者你不能改变系统的 ptrace 设置，你可以选择禁用 Vader BTL 的 CMA 支持。</p>
<p>collecting 1186 items                                                                                                                                       [duguex-NF5588M3:28524] 19 more processes have sent help message help-btl-vader.txt &#x2F; cma-permission-denied<br>[duguex-NF5588M3:28524] Set MCA parameter “orte_base_help_aggregate” to 0 to see all help &#x2F; error messages<br>collected 1856 items         </p>
<p>conda install -c conda-forge pytest pytest-xdist</p>
<p>Testing GPAW¶<br>Testing of gpaw is done by a nightly test suite consisting of many small and quick tests (with pytest) and by a weekly set of larger tests.</p>
<p>Test suite with pytest¶<br>The test suite consists of a large number of small and quick tests found in the gpaw&#x2F;test&#x2F; directory. The tests run nightly in serial and in parallel modes.</p>
<p>Running tests in serial mode¶<br>Use pytest to run the tests:</p>
<p>$ pytest –pyargs gpaw -v<br>To speed up the test suite, use pytest-xdist to use multiple processes to run multiple tests at the same time (note: each test is still run in serial mode):</p>
<p>$ pytest –pyargs gpaw -v -n <number-of-processes><br>Please report errors to the gpaw-users mailing list so that we can fix them (see Mail List).</p>
<p>Running tests in parallel mode<br>In order to run the tests with MPI parallelization, do this:</p>
<p>$ mpiexec -n 8 pytest –pyargs gpaw -v<br>The tests should pass with 1, 2, 4, and 8 parallel tasks.</p>
<p>mpiexec –mca opal_cuda_support 1</p>
<p>Hint</p>
<p>If you observe issues (e.g. segmentation faults) when trying to run pytest, try this instead:</p>
<p>$ mpiexec -n <n> gpaw python -m pytest –pyargs gpaw -v<br>This should ensure that the correct environment is used.</p>
<p>Please report also parallel errors to the mailing list so that we can fix them (see Mail List).</p>
<h1 id="gpaw-zfs"><a href="#gpaw-zfs" class="headerlink" title="gpaw zfs"></a>gpaw zfs</h1><p><a target="_blank" rel="noopener" href="https://wiki.fysik.dtu.dk/gpaw/tutorialsexercises/magnetic/zfs/zfs.html#gpaw.zero_field_splitting.zfs">https://wiki.fysik.dtu.dk/gpaw/tutorialsexercises/magnetic/zfs/zfs.html#gpaw.zero_field_splitting.zfs</a></p>
<p>gpaw.zero_field_splitting.zfs(calc, method&#x3D;1)[source]¶<br>Zero-field splitting.</p>
<p>Calculate magnetic dipole coupling tensor in eV.</p>
<p>gpaw.zero_field_splitting.convert_tensor(D_vv, unit&#x3D;’eV’)[source]¶<br>Convert 3x3 tensor to D, E and easy axis.</p>
<p>Input tensor must be in eV and the result can be returned in eV, μeV, MHz or 1&#x2F;cm according to the value of unit (must be one of “eV”, “ueV”, “MHz”, “1&#x2F;cm”).</p>
<p>D_vv &#x3D; np.diag([1, 2, 3])<br>D, E, axis, _ &#x3D; convert_tensor(D_vv)<br>D<br>4.5<br>E<br>0.5<br>axis<br>array([0., 0., 1.])</p>
<p>(gpaw) duguex@duguex3060:~&#x2F;gpaw$ gpaw -P 20 python diamond_nv_minus.py</p>
<p>import json<br>import numpy as np<br>from gpaw.zero_field_splitting import convert_tensor<br>a&#x3D;json.load(open(“zfs.json”))<br>convert_tensor(np.array(a[“D1”]).astype(float),unit&#x3D;’MHz’)</p>
<h1 id="pip-install-gpaw-instead-of-using-conda"><a href="#pip-install-gpaw-instead-of-using-conda" class="headerlink" title="pip install gpaw instead of using conda"></a>pip install gpaw instead of using conda</h1><p>for the usage in singularity container, pip installation is better</p>
<p>sudo apt install python3-dev libopenblas-dev libxc-dev libscalapack-mpi-dev libfftw3-dev python3-pip lrzsz nano<br>mkdir ~&#x2F;.gpaw&#x2F;<br>cd ~&#x2F;.gpaw&#x2F;<br>nano siteconfig.py</p>
<p>fftw &#x3D; True<br>scalapack &#x3D; True<br>libraries &#x3D; [‘xc’, ‘blas’, ‘fftw3’, ‘scalapack-openmpi’]<br>extra_compile_args +&#x3D; [‘-fopenmp’]<br>extra_link_args +&#x3D; [‘-fopenmp’]</p>
<p>pip install gpaw</p>
<p>gpaw install-data &#x2F;usr&#x2F;local&#x2F;share&#x2F;gpaw-setups</p>
<p>| PAW-datasets (1)  &#x2F;usr&#x2F;local&#x2F;share&#x2F;gpaw-setups                                                  |<br>| PAW-datasets (2)  &#x2F;usr&#x2F;share&#x2F;gpaw-setups   </p>
<p>export GPAW_SETUP_PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;share&#x2F;gpaw-setups-0.9.20000&#x2F;</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://tobedetermined.com/2024/06/15/network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Mingzhe Liu">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="TODO">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | TODO">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/06/15/network/" class="post-title-link" itemprop="url">network</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2024-06-15 21:24:29" itemprop="dateCreated datePublished" datetime="2024-06-15T21:24:29+08:00">2024-06-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-07-30 00:45:20" itemprop="dateModified" datetime="2024-07-30T00:45:20+08:00">2024-07-30</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="TL-WDR5620千兆版-在路由器上启用端口转移"><a href="#TL-WDR5620千兆版-在路由器上启用端口转移" class="headerlink" title="TL-WDR5620千兆版 在路由器上启用端口转移"></a>TL-WDR5620千兆版 在路由器上启用端口转移</h2><p>路由器ip地址114.214.207.59</p>
<h3 id="IP与MAC绑定"><a href="#IP与MAC绑定" class="headerlink" title="IP与MAC绑定"></a>IP与MAC绑定</h3><h3 id="虚拟服务器"><a href="#虚拟服务器" class="headerlink" title="虚拟服务器"></a>虚拟服务器</h3><p>常用服务器不用管它。<br>外部端口网上说尽量配一个9000以上的数字9090，否则可能会被网络供应商封禁，也不知道真假，宁可信其有吧。<br>内部端口网上说只能是80端口，但我测试了其他端口，照样有效。<br>IP地址，填写你所要配置的那台电脑的有线网卡的IP。<br>协议类型，填写ALL</p>
<p>Get-ItemProperty -Path ‘HKLM:\SYSTEM\CurrentControlSet\Control\Terminal Server\WinStations\RDP-Tcp’ -name “PortNumber”</p>
<p>PortNumber   : 3389<br>PSPath       : Microsoft.PowerShell.Core\Registry::HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Terminal Server\WinStations\RDP-Tcp<br>PSParentPath : Microsoft.PowerShell.Core\Registry::HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\Terminal Server\WinStations<br>PSChildName  : RDP-Tcp<br>PSDrive      : HKLM<br>PSProvider   : Microsoft.PowerShell.Core\Registry</p>
<p>$portvalue &#x3D; 3389</p>
<p>Set-ItemProperty -Path ‘HKLM:\SYSTEM\CurrentControlSet\Control\Terminal Server\WinStations\RDP-Tcp’ -name “PortNumber” -Value $portvalue </p>
<p>New-NetFirewallRule -DisplayName ‘RDPPORTLatest-TCP-In’ -Profile ‘Public’ -Direction Inbound -Action Allow -Protocol TCP -LocalPort $portvalue<br>New-NetFirewallRule -DisplayName ‘RDPPORTLatest-UDP-In’ -Profile ‘Public’ -Direction Inbound -Action Allow -Protocol UDP -LocalPort $portvalue</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75648110">https://zhuanlan.zhihu.com/p/75648110</a></p>
<p>MAC（Medium&#x2F;Media Access Control）地址，意译为媒体访问控制，或称为物理地址、硬件地址，用来定义网络设备的位置。 通常说的MAC地址指的是网卡的物理地址是由网卡生产厂家烧入网卡的EPROM（一种闪存芯片，通常可以通过程序擦写），它存储的是传输数据时真正赖以标识发出数据的电脑和接收数据的主机的地址。 在网络底层的物理传输过程中，是通过物理地址来识别主机的，它一定是全球唯一的。 比如，有线网卡，其物理地址是48bit（比特位）的整数，如：44-45-53-54-00-00,以机器可读的方式存入主机接口中。</p>
<p>duguex_2060	00-E0-4C-89-03-90	192.168.1.122</p>
<h1 id="rdp-远程桌面-连接microsoft账户"><a href="#rdp-远程桌面-连接microsoft账户" class="headerlink" title="rdp&#x2F;远程桌面 连接microsoft账户"></a>rdp&#x2F;远程桌面 连接microsoft账户</h1><p>打开远程桌面<br>关闭windows hello<br>win+l 切换密码登录<br>可以用联机账户&#x2F;密码登录</p>
<h1 id="临时服务器"><a href="#临时服务器" class="headerlink" title="临时服务器"></a>临时服务器</h1><p>你可以通过以下步骤将 Windows 中的文件发送到 WSL2 的 Docker 容器中：</p>
<ol>
<li><p><strong>在 Windows 中启动一个简单的 HTTP 服务器</strong>：你可以使用 Python 的 <code>http.server</code> 模块来启动一个简单的 HTTP 服务器。首先，打开一个命令提示符窗口，然后切换到包含你想要发送的文件的目录，然后运行以下命令：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m http.server</span><br></pre></td></tr></table></figure>

<p> 这个命令会在当前目录启动一个 HTTP 服务器，服务器的端口号是 8000。</p>
</li>
<li><p><strong>在 Docker 容器中下载文件</strong>：然后，你可以在 Docker 容器中使用 <code>curl</code> 或 <code>wget</code> 命令来下载文件。首先，打开一个终端窗口，然后运行以下命令来进入 Docker 容器：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it &lt;container-id&gt; /bin/bash</span><br></pre></td></tr></table></figure>

<p> 在这个命令中，<code>&lt;container-id&gt;</code> 是你的 Docker 容器的 ID。然后，你可以运行以下命令来下载文件：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://host.docker.internal:8000/&lt;filename&gt; -o &lt;filename&gt;</span><br></pre></td></tr></table></figure>

<p> 在这个命令中，<code>&lt;filename&gt;</code> 是你想要下载的文件的名称。<code>host.docker.internal</code> 是 Docker 容器中用来访问宿主机的特殊 DNS 名称。</p>
</li>
</ol>
<p>请注意，这个方法需要你的 Windows 系统和 Docker 容器都可以访问网络，并且你的 Windows 系统有 Python 环境。如果你的系统没有 Python 环境，你可以使用其他的方法来启动一个 HTTP 服务器，例如使用 Node.js 的 <code>http-server</code> 模块。</p>
<h1 id="nextcloud"><a href="#nextcloud" class="headerlink" title="nextcloud"></a>nextcloud</h1><p>docker run –name NextCloud -p 8080:80 <br>  -v &#x2F;home&#x2F;duguex&#x2F;nextcloud&#x2F;html&#x2F;:&#x2F;var&#x2F;www&#x2F;html <br>  -v &#x2F;home&#x2F;duguex&#x2F;nextcloud&#x2F;data&#x2F;:&#x2F;var&#x2F;www&#x2F;html&#x2F;data <br>  -d nextcloud</p>
<p>映射nextcloud_data文件夹（$PWD指的是当前文件夹，即nextcloud_data）下的data和html分别至容器的&#x2F;var&#x2F;www&#x2F;html&#x2F;data和&#x2F;var&#x2F;www&#x2F;html，从而避免将用户上传的文件和相关配置文件存储在容器中，方便后续管理。</p>
<p>如果希望NextCloud可以管理NAS的其他文件，需要配置NextCloud的外部存储。但这个功能默认是不加载的，所以要手动开启：完成安装后，点击头像-&gt;应用进入应用市场，在精选应用中找到官方的External storage support插件，点击启用。</p>
<p>开启后，点击头像-&gt;设置进入设置界面，在管理！管理！管理！选项卡下面，找到外部存储：<br><img src="/image-20.png" alt="Alt text"></p>
<h1 id="samba"><a href="#samba" class="headerlink" title="samba"></a>samba</h1><p>在 Ubuntu 中安装和配置 Samba 以及从 Windows 访问 Samba 共享的步骤如下：</p>
<ol>
<li>在 Ubuntu 中安装 Samba：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install samba</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>创建一个目录来共享：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /home/&lt;username&gt;/sambashare</span><br></pre></td></tr></table></figure>

<p>将 <code>&lt;username&gt;</code> 替换为你的用户名。</p>
<ol start="3">
<li>配置 Samba。编辑 Samba 的配置文件：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nano /etc/samba/smb.conf</span><br></pre></td></tr></table></figure>

<p>在文件的底部，添加以下内容：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[sambashare]</span></span><br><span class="line">    <span class="attr">comment</span> = Samba <span class="literal">on</span> Ubuntu</span><br><span class="line">    <span class="attr">path</span> = /home/&lt;username&gt;/sambashare</span><br><span class="line">    read <span class="attr">only</span> = <span class="literal">no</span></span><br><span class="line">    <span class="attr">browsable</span> = <span class="literal">yes</span></span><br></pre></td></tr></table></figure>

<p>将 <code>&lt;username&gt;</code> 替换为你的用户名。</p>
<ol start="4">
<li>重启 Samba 服务以应用更改：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service smbd restart</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>设置 Samba 密码：</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo smbpasswd -a &lt;username&gt;</span><br></pre></td></tr></table></figure>

<p>将 <code>&lt;username&gt;</code> 替换为你的用户名，然后输入你想要的密码。</p>
<p>现在，你应该可以从 Windows 访问你的 Samba 共享了。在 Windows 中，打开文件管理器，然后在地址栏输入 <code>\\&lt;samba_server_ip&gt;\sambashare</code>，将 <code>&lt;samba_server_ip&gt;</code> 替换为你的 Samba 服务器的 IP 地址。然后，你应该可以看到你的共享目录，并可以使用你在步骤 5 中设置的用户名和密码来访问它。</p>
<h1 id="浏览器行为自动化"><a href="#浏览器行为自动化" class="headerlink" title="浏览器行为自动化"></a>浏览器行为自动化</h1><p>conda install anaconda::bs4<br>conda install anaconda::requests</p>
<h1 id="通过-ssh-远程连接-wsl2"><a href="#通过-ssh-远程连接-wsl2" class="headerlink" title="通过 ssh 远程连接 wsl2"></a>通过 ssh 远程连接 wsl2</h1><h2 id="wsl上安装openssh"><a href="#wsl上安装openssh" class="headerlink" title="wsl上安装openssh"></a>wsl上安装openssh</h2><p>sudo apt install openssh-server</p>
<h2 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h2><p>sudo nano &#x2F;etc&#x2F;ssh&#x2F;sshd_config</p>
<p>Port 9022<br>PermitRootLogin yes<br>PasswordAuthentication yes</p>
<h2 id="允许ssh连接"><a href="#允许ssh连接" class="headerlink" title="允许ssh连接"></a>允许ssh连接</h2><p>sudo nano &#x2F;etc&#x2F;hosts.allow</p>
<p>添加 sshd: ALL</p>
<h2 id="重启ssh服务"><a href="#重启ssh服务" class="headerlink" title="重启ssh服务"></a>重启ssh服务</h2><p>sudo service ssh –full-restart<br>sudo systemctl restart ssh</p>
<h2 id="在windows中以管理员身份运行-进行端口转发"><a href="#在windows中以管理员身份运行-进行端口转发" class="headerlink" title="在windows中以管理员身份运行 进行端口转发"></a>在windows中以管理员身份运行 进行端口转发</h2><p>netsh interface portproxy add v4tov4 listenaddress&#x3D;0.0.0.0 listenport&#x3D;9022 connectaddress&#x3D;172.29.10.87 connectport&#x3D;9022</p>
<p>netsh interface portproxy add v4tov4 listenaddress&#x3D;0.0.0.0 listenport&#x3D;22 connectaddress&#x3D;172.31.24.96 connectport&#x3D;22</p>
<p>是的，<code>netsh interface portproxy add v4tov4</code>命令在Windows中设置的端口转发规则是永久的，即使重启计算机，这些规则也会保留。</p>
<p>这个命令的作用是在本地计算机（0.0.0.0:9022）和远程计算机（172.29.10.87:9022）之间创建一个端口转发规则。所有发送到本地计算机的9022端口的数据都会被转发到远程计算机的9022端口。</p>
<p>如果你想删除这个规则，你可以使用以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netsh interface portproxy delete v4tov4 listenaddress=0.0.0.0 listenport=9022</span><br></pre></td></tr></table></figure>

<p>这个命令会删除之前设置的端口转发规则。</p>
<p>在Windows中，你可以使用<code>netsh</code>命令来查看端口转发规则。以下是具体步骤：</p>
<ol>
<li><p>打开命令提示符。你可以按Win+R键，然后输入<code>cmd</code>并按Enter键来打开命令提示符。</p>
</li>
<li><p>在命令提示符中，输入以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netsh interface portproxy show all</span><br></pre></td></tr></table></figure></li>
</ol>
<p>这个命令会显示所有的端口转发规则。每个规则都会显示监听地址（listenaddress）、监听端口（listenport）、连接地址（connectaddress）和连接端口（connectport）。</p>
<h2 id="防火墙开放端口"><a href="#防火墙开放端口" class="headerlink" title="防火墙开放端口"></a>防火墙开放端口</h2><p>netsh advfirewall firewall add rule name&#x3D;WSL2 dir&#x3D;in action&#x3D;allow protocol&#x3D;TCP localport&#x3D;9022</p>
<h1 id="what-is-the-difference-between-127-0-0-1-localhost-and-0-0-0-0"><a href="#what-is-the-difference-between-127-0-0-1-localhost-and-0-0-0-0" class="headerlink" title="what is the difference between 127.0.0.1 localhost and 0.0.0.0?"></a>what is the difference between 127.0.0.1 localhost and 0.0.0.0?</h1><p><code>127.0.0.1</code> 和 <code>0.0.0.0</code> 都是特殊的 IP 地址，但它们的用途不同。</p>
<p><code>127.0.0.1</code> 是一个特殊的 IP 地址，通常被称为本地回环地址或 localhost。当一个程序需要与运行在同一台机器上的其他程序进行通信时，就会使用这个地址。例如，如果你在你的机器上运行一个 web 服务器，并且在同一台机器的浏览器中访问它，你就可以使用 <a target="_blank" rel="noopener" href="http://127.0.0.1/">http://127.0.0.1</a> 来访问。</p>
<p><code>0.0.0.0</code> 在网络编程中有特殊的含义。当一个网络程序绑定到 <code>0.0.0.0</code> 时，它表示该程序正在监听所有可用的 IP 地址。在服务器环境中，如果一个服务被设置为监听 <code>0.0.0.0</code>，那么无论通过该服务器的哪个 IP 地址，都可以访问到这个服务。</p>
<p>总的来说，<code>127.0.0.1</code> 是用于本地机器内部通信的 IP 地址，而 <code>0.0.0.0</code> 是用于表示 “所有 IP 地址” 或 “所有网络接口” 的地址。</p>
<h1 id="一台机器可以有多个ip吗"><a href="#一台机器可以有多个ip吗" class="headerlink" title="一台机器可以有多个ip吗"></a>一台机器可以有多个ip吗</h1><p>是的，一台机器可以有多个IP地址。这种情况通常在以下几种情况下发生：</p>
<p>多网卡：如果一台机器有多个网络接口（例如，多个以太网卡或无线网卡），那么每个网络接口都可以有一个或多个IP地址。</p>
<p>虚拟网络接口：一台机器可以创建多个虚拟网络接口，每个接口都可以有自己的IP地址。这在虚拟化和容器化环境中很常见。</p>
<p>IP别名：一台机器的单个网络接口可以配置多个IP地址。这被称为IP别名。</p>
<p>VPN和隧道：通过VPN或网络隧道，一台机器可以获得额外的IP地址。</p>
<p>IPv4和IPv6：一台机器可以同时有IPv4和IPv6地址。</p>
<p>这种能力使得一台机器可以在多个网络中同时操作，或者提供多个网络服务，每个服务都有自己的IP地址。</p>
<h1 id="复制私钥需要包含头尾"><a href="#复制私钥需要包含头尾" class="headerlink" title="复制私钥需要包含头尾"></a>复制私钥需要包含头尾</h1><h1 id="Config-Network-File-System-NFS"><a href="#Config-Network-File-System-NFS" class="headerlink" title="Config Network File System (NFS)"></a>Config Network File System (NFS)</h1><h2 id="for-server"><a href="#for-server" class="headerlink" title="for server"></a>for server</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-kernel-server</span><br><span class="line">sudo systemctl start nfs-kernel-server.service</span><br></pre></td></tr></table></figure>
<p>You can configure the directories to be exported by adding them to the &#x2F;etc&#x2F;exports file. For example:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/srv     *(ro,<span class="built_in">sync</span>,subtree_check)</span><br><span class="line">/home    *.hostname.com(rw,<span class="built_in">sync</span>,no_subtree_check)</span><br><span class="line">/scratch *(rw,async,no_subtree_check,no_root_squash)</span><br><span class="line">/home/duguex/vasp 192.168.1.111(rw,<span class="built_in">sync</span>,no_subtree_check)</span><br><span class="line">/home/duguex/rc-tmp 192.168.1.111(rw,<span class="built_in">sync</span>,no_subtree_check)</span><br></pre></td></tr></table></figure>
<p>Make sure any custom mount points you’re adding have been created (&#x2F;srv and &#x2F;home will already exist):</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /scratch</span><br><span class="line"><span class="built_in">mkdir</span> /home/duguex/vasp</span><br><span class="line"><span class="built_in">mkdir</span> /home/duguex/rc-tmp</span><br></pre></td></tr></table></figure>
<p>Apply the new config via:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo exportfs -a</span><br></pre></td></tr></table></figure>
<h2 id="for-client"><a href="#for-client" class="headerlink" title="for client"></a>for client</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install nfs-common</span><br><span class="line">sudo <span class="built_in">mkdir</span> /home/duguex/rc-tmp</span><br><span class="line">sudo mount 192.168.1.100:/home/duguex/rc-tmp /home/duguex/rc-tmp; </span><br><span class="line">sudo mount 192.168.1.100:/home/duguex/container /home/duguex/container</span><br><span class="line"></span><br><span class="line">sudo mount 192.168.1.122:/home/duguex/rc-tmp /home/duguex/rc-tmp</span><br><span class="line">sudo mount 192.168.1.122:/home/duguex/container /home/duguex/container</span><br><span class="line">sudo mount 192.168.1.122:/home/jjcainew /home/jjcainew</span><br></pre></td></tr></table></figure>

<p>You can replace * with one of the hostname formats. Make the hostname declaration as specific as possible so unwanted systems cannot access the NFS mount. Be aware that *.hostname.com will match foo.hostname.com but not foo.bar.my-domain.com.</p>
<p>The sync&#x2F;async options control whether changes are gauranteed to be committed to stable storage before replying to requests. async thus gives a performance benefit but risks data loss or corruption. Even though sync is the default, it’s worth setting since exportfs will issue a warning if it’s left unspecified.</p>
<p>subtree_check and no_subtree_check enables or disables a security verification that subdirectories a client attempts to mount for an exported filesystem are ones they’re permitted to do so. This verification step has some performance implications for some use cases, such as home directories with frequent file renames. Read-only filesystems are more suitable to enable subtree_check on. Like with sync, exportfs will warn if it’s left unspecified.</p>
<p>There are a number of optional settings for NFS mounts for tuning performance, tightening security, or providing conveniences. These settings each have their own trade-offs so it is important to use them with care, only as needed for the particular use case. no_root_squash, for example, adds a convenience to allow root-owned files to be modified by any client system’s root user; in a multi-user environment where executables are allowed on a shared mount point, this could lead to security problems.</p>
<p>你可以将 * 替换为其中一种主机名格式。尽可能具体地声明主机名，以防止不需要的系统访问 NFS 挂载。请注意，*.hostname.com 将匹配 foo.hostname.com，但不会匹配 foo.bar.my-domain.com。</p>
<p>sync&#x2F;async 选项控制是否在回复请求之前确保更改已提交到稳定存储。因此，async 提供了性能优势，但存在数据丢失或损坏的风险。尽管 sync 是默认设置，但由于如果未指定，exportfs 将发出警告，因此值得设置。</p>
<p>subtree_check 和 no_subtree_check 启用或禁用一项安全验证，即客户端尝试为导出的文件系统挂载的子目录是否是它们被允许的子目录。这个验证步骤对于一些使用情况（如频繁重命名文件的主目录）有一些性能影响。只读文件系统更适合在上面启用 subtree_check。像 sync 一样，如果未指定，exportfs 会发出警告。</p>
<p>NFS 挂载有许多可选设置，用于调整性能、加强安全性或提供便利。这些设置各有利弊，所以使用它们时要小心，只根据特定的使用情况需要使用。例如，no_root_squash 增加了一种便利，允许任何客户端系统的 root 用户修改 root 所有的文件；在一个允许在共享挂载点上执行可执行文件的多用户环境中，这可能导致安全问题。</p>
<p>sudo apt-get install ibutils<br>sudo apt install infiniband-diags</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" title="Next page" aria-label="Next page" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Mingzhe Liu</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
